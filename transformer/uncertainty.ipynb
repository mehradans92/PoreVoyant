{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
      "3.10.12\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "import platform\n",
    "print(platform.python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch import nn, Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sartaaj/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from MOFormer_modded.transformer import Transformer, TransformerRegressor\n",
    "from MOFormer_modded.dataset_modded import MOF_ID_Dataset\n",
    "from MOFormer_modded.tokenizer.mof_tokenizer import MOFTokenizer\n",
    "import yaml\n",
    "from MOFormer_modded.model.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_446781/1200223640.py:1: DtypeWarning: Columns (40,41,43,44,45,46,47,49,50,51,52,53,55,56,57,65,66,67,68,69,77,78,79,80,81,89,90,91,92,93) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  qmof_df = pd.read_csv(\"qmof.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qmof_id</th>\n",
       "      <th>name</th>\n",
       "      <th>info.formula</th>\n",
       "      <th>info.formula_reduced</th>\n",
       "      <th>info.mofid.mofid</th>\n",
       "      <th>info.mofid.mofkey</th>\n",
       "      <th>info.mofid.smiles_nodes</th>\n",
       "      <th>info.mofid.smiles_linkers</th>\n",
       "      <th>info.mofid.smiles</th>\n",
       "      <th>info.mofid.topology</th>\n",
       "      <th>...</th>\n",
       "      <th>outputs.hse06.energy_elec</th>\n",
       "      <th>outputs.hse06.net_magmom</th>\n",
       "      <th>outputs.hse06.bandgap</th>\n",
       "      <th>outputs.hse06.cbm</th>\n",
       "      <th>outputs.hse06.vbm</th>\n",
       "      <th>outputs.hse06.directgap</th>\n",
       "      <th>outputs.hse06.bandgap_spins</th>\n",
       "      <th>outputs.hse06.cbm_spins</th>\n",
       "      <th>outputs.hse06.vbm_spins</th>\n",
       "      <th>outputs.hse06.directgap_spins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qmof-8a95c27</td>\n",
       "      <td>ABACUF01_FSR</td>\n",
       "      <td>Ba2CuC6H14O16</td>\n",
       "      <td>Ba2CuC6H14O16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['O', '[Ba]', '[Cu]']</td>\n",
       "      <td>['[O-]C=O']</td>\n",
       "      <td>O.[Ba].[Cu].[O-]C=O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qmof-019ba28</td>\n",
       "      <td>ABALOF_FSR</td>\n",
       "      <td>Cu12C36H56I16N4S4</td>\n",
       "      <td>Cu3C9H14I4NS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qmof-830ed1c</td>\n",
       "      <td>ABAVIJ_FSR</td>\n",
       "      <td>Co4C48H32N8O16</td>\n",
       "      <td>CoC12H8N2O4</td>\n",
       "      <td>[Co].[O-]C(=O)c1ccncc1 MOFid-v1.rtl.cat0</td>\n",
       "      <td>Co.TWBYWOBDOCUKOW.MOFkey-v1.rtl</td>\n",
       "      <td>['[Co]']</td>\n",
       "      <td>['[O-]C(=O)c1ccncc1']</td>\n",
       "      <td>[Co].[O-]C(=O)c1ccncc1</td>\n",
       "      <td>rtl</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qmof-5bd4a24</td>\n",
       "      <td>ABAVOP_FSR</td>\n",
       "      <td>Co4C48H32N8O16</td>\n",
       "      <td>CoC12H8N2O4</td>\n",
       "      <td>[Co].[O-]C(=O)c1ccncc1 MOFid-v1.rtl.cat0</td>\n",
       "      <td>Co.TWBYWOBDOCUKOW.MOFkey-v1.rtl</td>\n",
       "      <td>['[Co]']</td>\n",
       "      <td>['[O-]C(=O)c1ccncc1']</td>\n",
       "      <td>[Co].[O-]C(=O)c1ccncc1</td>\n",
       "      <td>rtl</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qmof-644aab4</td>\n",
       "      <td>ABAXUZ_FSR</td>\n",
       "      <td>Zn2C50H32N6O8S4</td>\n",
       "      <td>ZnC25H16N3O4S2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['[Zn][Zn]']</td>\n",
       "      <td>['[O-]C(=O)c1cccc(c1)c1nccs1', 'n1ccc(cc1)c1cc...</td>\n",
       "      <td>[O-]C(=O)c1cccc(c1)c1nccs1.[Zn][Zn].n1ccc(cc1)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-811.553858</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.901747</td>\n",
       "      <td>2.246703</td>\n",
       "      <td>-0.655044</td>\n",
       "      <td>True</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>[None, None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20370</th>\n",
       "      <td>qmof-7aebbbb</td>\n",
       "      <td>tobacco_srsb_sym_3_on_2_sym_3_mc_0_L_2</td>\n",
       "      <td>Cu12C84H60N24</td>\n",
       "      <td>CuC7H5N2</td>\n",
       "      <td>N1=C[C](C=N1)C=Cc1cc(C=CC2=C[N]N=C2)cc(c1)C=CC...</td>\n",
       "      <td>Cu.IBPUNEAULYEGJU.MOFkey-v1.srs</td>\n",
       "      <td>['[Cu]']</td>\n",
       "      <td>['N1=C[C](C=N1)C=Cc1cc(C=CC2=C[N]N=C2)cc(c1)C=...</td>\n",
       "      <td>N1=C[C](C=N1)C=Cc1cc(C=CC2=C[N]N=C2)cc(c1)C=CC...</td>\n",
       "      <td>srs</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20371</th>\n",
       "      <td>qmof-9a04c15</td>\n",
       "      <td>tobacco_srsb_sym_3_on_2_sym_3_mc_0_L_6</td>\n",
       "      <td>Cu12C84H48N60</td>\n",
       "      <td>CuC7H4N5</td>\n",
       "      <td>N1=C[C](C=N1)n1nnc(c1)c1cc(cc(c1)c1nnn(c1)C1=C...</td>\n",
       "      <td>Cu.JWLDCPHWRGZUAB.MOFkey-v1.srs</td>\n",
       "      <td>['[Cu]']</td>\n",
       "      <td>['N1=C[C](C=N1)n1nnc(c1)c1cc(cc(c1)c1nnn(c1)C1...</td>\n",
       "      <td>N1=C[C](C=N1)n1nnc(c1)c1cc(cc(c1)c1nnn(c1)C1=C...</td>\n",
       "      <td>srs</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20372</th>\n",
       "      <td>qmof-0dce90f</td>\n",
       "      <td>tobacco_srsb_sym_3_on_2_sym_3_mc_0__</td>\n",
       "      <td>Cu12C60H36N24</td>\n",
       "      <td>CuC5H3N2</td>\n",
       "      <td>N1=C[C](C=N1)c1cc(cc(c1)C1=CN=N[CH]1)C1=C[N]N=...</td>\n",
       "      <td>Cu.PJSMFZDMZONQKK.MOFkey-v1.srs</td>\n",
       "      <td>['[Cu]', '[Cu][Cu]']</td>\n",
       "      <td>['N1=C[C](C=N1)c1cc(cc(c1)C1=CN=N[CH]1)C1=C[N]...</td>\n",
       "      <td>N1=C[C](C=N1)c1cc(cc(c1)C1=CN=N[CH]1)C1=C[N]N=...</td>\n",
       "      <td>srs</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20373</th>\n",
       "      <td>qmof-955fe88</td>\n",
       "      <td>tobacco_srsb_sym_3_on_4_sym_3_mc_0_L_2</td>\n",
       "      <td>Cu12C112H72N24</td>\n",
       "      <td>Cu3C28H18N6</td>\n",
       "      <td>N1=C[C](C=N1)C=CC1=CC2=CC(=CC3=CC(=CC(=C1)[C]2...</td>\n",
       "      <td>Cu.WCJPEIPZJUESBA.MOFkey-v1.srs</td>\n",
       "      <td>['[Cu]']</td>\n",
       "      <td>['N1=C[C](C=N1)C=CC1=CC2=CC(=CC3=CC(=CC(=C1)[C...</td>\n",
       "      <td>N1=C[C](C=N1)C=CC1=CC2=CC(=CC3=CC(=CC(=C1)[C]2...</td>\n",
       "      <td>srs</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20374</th>\n",
       "      <td>qmof-6538047</td>\n",
       "      <td>tobacco_srsb_sym_3_on_5_sym_3_mc_0_L_2</td>\n",
       "      <td>Cu12C108H72N36</td>\n",
       "      <td>CuC9H6N3</td>\n",
       "      <td>N1=C[C](C=N1)C=Cn1cc2c(c1)c1cn(cc1c1c2cn(c1)C=...</td>\n",
       "      <td>Cu.HPUZBJXKDGSLPP.MOFkey-v1.srs</td>\n",
       "      <td>['[Cu]', '[Cu][Cu]']</td>\n",
       "      <td>['N1=C[C](C=N1)C=Cn1cc2c(c1)c1cn(cc1c1c2cn(c1)...</td>\n",
       "      <td>N1=C[C](C=N1)C=Cn1cc2c(c1)c1cn(cc1c1c2cn(c1)C=...</td>\n",
       "      <td>srs</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20375 rows × 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            qmof_id                                    name  \\\n",
       "0      qmof-8a95c27                            ABACUF01_FSR   \n",
       "1      qmof-019ba28                              ABALOF_FSR   \n",
       "2      qmof-830ed1c                              ABAVIJ_FSR   \n",
       "3      qmof-5bd4a24                              ABAVOP_FSR   \n",
       "4      qmof-644aab4                              ABAXUZ_FSR   \n",
       "...             ...                                     ...   \n",
       "20370  qmof-7aebbbb  tobacco_srsb_sym_3_on_2_sym_3_mc_0_L_2   \n",
       "20371  qmof-9a04c15  tobacco_srsb_sym_3_on_2_sym_3_mc_0_L_6   \n",
       "20372  qmof-0dce90f    tobacco_srsb_sym_3_on_2_sym_3_mc_0__   \n",
       "20373  qmof-955fe88  tobacco_srsb_sym_3_on_4_sym_3_mc_0_L_2   \n",
       "20374  qmof-6538047  tobacco_srsb_sym_3_on_5_sym_3_mc_0_L_2   \n",
       "\n",
       "            info.formula info.formula_reduced  \\\n",
       "0          Ba2CuC6H14O16        Ba2CuC6H14O16   \n",
       "1      Cu12C36H56I16N4S4         Cu3C9H14I4NS   \n",
       "2         Co4C48H32N8O16          CoC12H8N2O4   \n",
       "3         Co4C48H32N8O16          CoC12H8N2O4   \n",
       "4        Zn2C50H32N6O8S4       ZnC25H16N3O4S2   \n",
       "...                  ...                  ...   \n",
       "20370      Cu12C84H60N24             CuC7H5N2   \n",
       "20371      Cu12C84H48N60             CuC7H4N5   \n",
       "20372      Cu12C60H36N24             CuC5H3N2   \n",
       "20373     Cu12C112H72N24          Cu3C28H18N6   \n",
       "20374     Cu12C108H72N36             CuC9H6N3   \n",
       "\n",
       "                                        info.mofid.mofid  \\\n",
       "0                                                    NaN   \n",
       "1                                                    NaN   \n",
       "2               [Co].[O-]C(=O)c1ccncc1 MOFid-v1.rtl.cat0   \n",
       "3               [Co].[O-]C(=O)c1ccncc1 MOFid-v1.rtl.cat0   \n",
       "4                                                    NaN   \n",
       "...                                                  ...   \n",
       "20370  N1=C[C](C=N1)C=Cc1cc(C=CC2=C[N]N=C2)cc(c1)C=CC...   \n",
       "20371  N1=C[C](C=N1)n1nnc(c1)c1cc(cc(c1)c1nnn(c1)C1=C...   \n",
       "20372  N1=C[C](C=N1)c1cc(cc(c1)C1=CN=N[CH]1)C1=C[N]N=...   \n",
       "20373  N1=C[C](C=N1)C=CC1=CC2=CC(=CC3=CC(=CC(=C1)[C]2...   \n",
       "20374  N1=C[C](C=N1)C=Cn1cc2c(c1)c1cn(cc1c1c2cn(c1)C=...   \n",
       "\n",
       "                     info.mofid.mofkey info.mofid.smiles_nodes  \\\n",
       "0                                  NaN   ['O', '[Ba]', '[Cu]']   \n",
       "1                                  NaN                     NaN   \n",
       "2      Co.TWBYWOBDOCUKOW.MOFkey-v1.rtl                ['[Co]']   \n",
       "3      Co.TWBYWOBDOCUKOW.MOFkey-v1.rtl                ['[Co]']   \n",
       "4                                  NaN            ['[Zn][Zn]']   \n",
       "...                                ...                     ...   \n",
       "20370  Cu.IBPUNEAULYEGJU.MOFkey-v1.srs                ['[Cu]']   \n",
       "20371  Cu.JWLDCPHWRGZUAB.MOFkey-v1.srs                ['[Cu]']   \n",
       "20372  Cu.PJSMFZDMZONQKK.MOFkey-v1.srs    ['[Cu]', '[Cu][Cu]']   \n",
       "20373  Cu.WCJPEIPZJUESBA.MOFkey-v1.srs                ['[Cu]']   \n",
       "20374  Cu.HPUZBJXKDGSLPP.MOFkey-v1.srs    ['[Cu]', '[Cu][Cu]']   \n",
       "\n",
       "                               info.mofid.smiles_linkers  \\\n",
       "0                                            ['[O-]C=O']   \n",
       "1                                                    NaN   \n",
       "2                                  ['[O-]C(=O)c1ccncc1']   \n",
       "3                                  ['[O-]C(=O)c1ccncc1']   \n",
       "4      ['[O-]C(=O)c1cccc(c1)c1nccs1', 'n1ccc(cc1)c1cc...   \n",
       "...                                                  ...   \n",
       "20370  ['N1=C[C](C=N1)C=Cc1cc(C=CC2=C[N]N=C2)cc(c1)C=...   \n",
       "20371  ['N1=C[C](C=N1)n1nnc(c1)c1cc(cc(c1)c1nnn(c1)C1...   \n",
       "20372  ['N1=C[C](C=N1)c1cc(cc(c1)C1=CN=N[CH]1)C1=C[N]...   \n",
       "20373  ['N1=C[C](C=N1)C=CC1=CC2=CC(=CC3=CC(=CC(=C1)[C...   \n",
       "20374  ['N1=C[C](C=N1)C=Cn1cc2c(c1)c1cn(cc1c1c2cn(c1)...   \n",
       "\n",
       "                                       info.mofid.smiles info.mofid.topology  \\\n",
       "0                                    O.[Ba].[Cu].[O-]C=O                 NaN   \n",
       "1                                                    NaN                 NaN   \n",
       "2                                 [Co].[O-]C(=O)c1ccncc1                 rtl   \n",
       "3                                 [Co].[O-]C(=O)c1ccncc1                 rtl   \n",
       "4      [O-]C(=O)c1cccc(c1)c1nccs1.[Zn][Zn].n1ccc(cc1)...                 NaN   \n",
       "...                                                  ...                 ...   \n",
       "20370  N1=C[C](C=N1)C=Cc1cc(C=CC2=C[N]N=C2)cc(c1)C=CC...                 srs   \n",
       "20371  N1=C[C](C=N1)n1nnc(c1)c1cc(cc(c1)c1nnn(c1)C1=C...                 srs   \n",
       "20372  N1=C[C](C=N1)c1cc(cc(c1)C1=CN=N[CH]1)C1=C[N]N=...                 srs   \n",
       "20373  N1=C[C](C=N1)C=CC1=CC2=CC(=CC3=CC(=CC(=C1)[C]2...                 srs   \n",
       "20374  N1=C[C](C=N1)C=Cn1cc2c(c1)c1cn(cc1c1c2cn(c1)C=...                 srs   \n",
       "\n",
       "       ...  outputs.hse06.energy_elec  outputs.hse06.net_magmom  \\\n",
       "0      ...                        NaN                       NaN   \n",
       "1      ...                        NaN                       NaN   \n",
       "2      ...                        NaN                       NaN   \n",
       "3      ...                        NaN                       NaN   \n",
       "4      ...                -811.553858                       0.0   \n",
       "...    ...                        ...                       ...   \n",
       "20370  ...                        NaN                       NaN   \n",
       "20371  ...                        NaN                       NaN   \n",
       "20372  ...                        NaN                       NaN   \n",
       "20373  ...                        NaN                       NaN   \n",
       "20374  ...                        NaN                       NaN   \n",
       "\n",
       "       outputs.hse06.bandgap  outputs.hse06.cbm  outputs.hse06.vbm  \\\n",
       "0                        NaN                NaN                NaN   \n",
       "1                        NaN                NaN                NaN   \n",
       "2                        NaN                NaN                NaN   \n",
       "3                        NaN                NaN                NaN   \n",
       "4                   2.901747           2.246703          -0.655044   \n",
       "...                      ...                ...                ...   \n",
       "20370                    NaN                NaN                NaN   \n",
       "20371                    NaN                NaN                NaN   \n",
       "20372                    NaN                NaN                NaN   \n",
       "20373                    NaN                NaN                NaN   \n",
       "20374                    NaN                NaN                NaN   \n",
       "\n",
       "      outputs.hse06.directgap  outputs.hse06.bandgap_spins  \\\n",
       "0                         NaN                          NaN   \n",
       "1                         NaN                          NaN   \n",
       "2                         NaN                          NaN   \n",
       "3                         NaN                          NaN   \n",
       "4                        True                 [None, None]   \n",
       "...                       ...                          ...   \n",
       "20370                     NaN                          NaN   \n",
       "20371                     NaN                          NaN   \n",
       "20372                     NaN                          NaN   \n",
       "20373                     NaN                          NaN   \n",
       "20374                     NaN                          NaN   \n",
       "\n",
       "      outputs.hse06.cbm_spins outputs.hse06.vbm_spins  \\\n",
       "0                         NaN                     NaN   \n",
       "1                         NaN                     NaN   \n",
       "2                         NaN                     NaN   \n",
       "3                         NaN                     NaN   \n",
       "4                [None, None]            [None, None]   \n",
       "...                       ...                     ...   \n",
       "20370                     NaN                     NaN   \n",
       "20371                     NaN                     NaN   \n",
       "20372                     NaN                     NaN   \n",
       "20373                     NaN                     NaN   \n",
       "20374                     NaN                     NaN   \n",
       "\n",
       "       outputs.hse06.directgap_spins  \n",
       "0                                NaN  \n",
       "1                                NaN  \n",
       "2                                NaN  \n",
       "3                                NaN  \n",
       "4                       [None, None]  \n",
       "...                              ...  \n",
       "20370                            NaN  \n",
       "20371                            NaN  \n",
       "20372                            NaN  \n",
       "20373                            NaN  \n",
       "20374                            NaN  \n",
       "\n",
       "[20375 rows x 94 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qmof_df = pd.read_csv(\"qmof.csv\")\n",
    "qmof_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.632527\n",
       "1        1.134232\n",
       "2        0.345448\n",
       "3        0.342645\n",
       "4        1.973007\n",
       "           ...   \n",
       "20370    2.692705\n",
       "20371    3.326284\n",
       "20372    3.383629\n",
       "20373    0.660589\n",
       "20374    2.348021\n",
       "Name: outputs.pbe.bandgap, Length: 20375, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qmof_df['outputs.pbe.bandgap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MOFid</th>\n",
       "      <th>Band Gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[O-]C(=O)c1cc2c3c(c1)c(N(=O)=O)c(c1c3c(c(c2N(=...</td>\n",
       "      <td>2.201034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cl[Cd].O=C1[N]NC(=O)c2c1nccc2&amp;&amp;fes.cat0</td>\n",
       "      <td>2.055500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Zn].c1ccc2c(c1)N=C[N]2&amp;&amp;sod.cat0</td>\n",
       "      <td>3.677709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Cu].[O-]C(=O)CCc1cccnc1&amp;&amp;sql.cat0</td>\n",
       "      <td>0.959976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fc1cncc(c1C#Cc1c(F)cncc1F)F.[O-]C(=O)c1c(Cl)c(...</td>\n",
       "      <td>1.098963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7461</th>\n",
       "      <td>[O-]C(=O)c1cc(cc(c1)N(=O)=O)C(=O)[O-].[Zn].c1n...</td>\n",
       "      <td>2.038372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7462</th>\n",
       "      <td>O=C(c1ccncc1)N1CCN(CC1)C(=O)c1ccncc1.[O-]C(=O)...</td>\n",
       "      <td>1.747641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7463</th>\n",
       "      <td>COc1cc2ccc3c(c2cc1C(=O)[O-])ccc1c3cc(C(=O)[O-]...</td>\n",
       "      <td>1.665536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7464</th>\n",
       "      <td>[Ni].c1ccc(cn1)[CH][N][N][CH]c1cccnc1&amp;&amp;cdt.cat0</td>\n",
       "      <td>1.388292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7465</th>\n",
       "      <td>[Cd].[O-]C(=O)c1cccc(c1)C(=O)[O-]&amp;&amp;sql.cat1</td>\n",
       "      <td>3.172720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7466 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  MOFid  Band Gap\n",
       "0     [O-]C(=O)c1cc2c3c(c1)c(N(=O)=O)c(c1c3c(c(c2N(=...  2.201034\n",
       "1               Cl[Cd].O=C1[N]NC(=O)c2c1nccc2&&fes.cat0  2.055500\n",
       "2                     [Zn].c1ccc2c(c1)N=C[N]2&&sod.cat0  3.677709\n",
       "3                    [Cu].[O-]C(=O)CCc1cccnc1&&sql.cat0  0.959976\n",
       "4     Fc1cncc(c1C#Cc1c(F)cncc1F)F.[O-]C(=O)c1c(Cl)c(...  1.098963\n",
       "...                                                 ...       ...\n",
       "7461  [O-]C(=O)c1cc(cc(c1)N(=O)=O)C(=O)[O-].[Zn].c1n...  2.038372\n",
       "7462  O=C(c1ccncc1)N1CCN(CC1)C(=O)c1ccncc1.[O-]C(=O)...  1.747641\n",
       "7463  COc1cc2ccc3c(c2cc1C(=O)[O-])ccc1c3cc(C(=O)[O-]...  1.665536\n",
       "7464    [Ni].c1ccc(cn1)[CH][N][N][CH]c1cccnc1&&cdt.cat0  1.388292\n",
       "7465        [Cd].[O-]C(=O)c1cccc(c1)C(=O)[O-]&&sql.cat1  3.172720\n",
       "\n",
       "[7466 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qmof_mofid = pd.read_csv(\"QMOF_small_mofid.csv\", header = None)\n",
    "qmof_mofid.columns = ['MOFid', 'Band Gap']\n",
    "qmof_mofid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_smiles_stable = {'qmof_ID' : [],\n",
    "                    'SMILES' : [],\n",
    "                    'Band Gap' : []\n",
    "                    }\n",
    "\n",
    "for i in qmof_mofid['MOFid'].values:\n",
    "    smiles = i.split('&&')[0]\n",
    "    try:\n",
    "        qmof_id = qmof_df[qmof_df['info.mofid.smiles'] == smiles]['qmof_id'].values[0]\n",
    "        bandgap = qmof_mofid[qmof_mofid['MOFid'] == i]['Band Gap'].values[0]\n",
    "\n",
    "        id_smiles_stable['SMILES'].append(smiles)\n",
    "        id_smiles_stable['qmof_ID'].append(qmof_id)\n",
    "        id_smiles_stable['Band Gap'].append(bandgap)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qmof_ID</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>Band Gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qmof-02f9568</td>\n",
       "      <td>[O-]C(=O)c1cc2c3c(c1)c(N(=O)=O)c(c1c3c(c(c2N(=...</td>\n",
       "      <td>2.201034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qmof-f8ab6e6</td>\n",
       "      <td>Cl[Cd].O=C1[N]NC(=O)c2c1nccc2</td>\n",
       "      <td>2.055500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qmof-ac208a4</td>\n",
       "      <td>[Zn].c1ccc2c(c1)N=C[N]2</td>\n",
       "      <td>3.677709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qmof-d967b7b</td>\n",
       "      <td>[Cu].[O-]C(=O)CCc1cccnc1</td>\n",
       "      <td>0.959976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qmof-c7f2375</td>\n",
       "      <td>Fc1cncc(c1C#Cc1c(F)cncc1F)F.[O-]C(=O)c1c(Cl)c(...</td>\n",
       "      <td>1.098963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7185</th>\n",
       "      <td>qmof-87a0280</td>\n",
       "      <td>[O-]C(=O)c1cc(cc(c1)N(=O)=O)C(=O)[O-].[Zn].c1n...</td>\n",
       "      <td>2.038372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7186</th>\n",
       "      <td>qmof-c645675</td>\n",
       "      <td>O=C(c1ccncc1)N1CCN(CC1)C(=O)c1ccncc1.[O-]C(=O)...</td>\n",
       "      <td>1.747641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7187</th>\n",
       "      <td>qmof-b2c7c73</td>\n",
       "      <td>COc1cc2ccc3c(c2cc1C(=O)[O-])ccc1c3cc(C(=O)[O-]...</td>\n",
       "      <td>1.665536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7188</th>\n",
       "      <td>qmof-fcb8885</td>\n",
       "      <td>[Ni].c1ccc(cn1)[CH][N][N][CH]c1cccnc1</td>\n",
       "      <td>1.388292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7189</th>\n",
       "      <td>qmof-d8acb74</td>\n",
       "      <td>[Cd].[O-]C(=O)c1cccc(c1)C(=O)[O-]</td>\n",
       "      <td>3.172720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7190 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           qmof_ID                                             SMILES  \\\n",
       "0     qmof-02f9568  [O-]C(=O)c1cc2c3c(c1)c(N(=O)=O)c(c1c3c(c(c2N(=...   \n",
       "1     qmof-f8ab6e6                      Cl[Cd].O=C1[N]NC(=O)c2c1nccc2   \n",
       "2     qmof-ac208a4                            [Zn].c1ccc2c(c1)N=C[N]2   \n",
       "3     qmof-d967b7b                           [Cu].[O-]C(=O)CCc1cccnc1   \n",
       "4     qmof-c7f2375  Fc1cncc(c1C#Cc1c(F)cncc1F)F.[O-]C(=O)c1c(Cl)c(...   \n",
       "...            ...                                                ...   \n",
       "7185  qmof-87a0280  [O-]C(=O)c1cc(cc(c1)N(=O)=O)C(=O)[O-].[Zn].c1n...   \n",
       "7186  qmof-c645675  O=C(c1ccncc1)N1CCN(CC1)C(=O)c1ccncc1.[O-]C(=O)...   \n",
       "7187  qmof-b2c7c73  COc1cc2ccc3c(c2cc1C(=O)[O-])ccc1c3cc(C(=O)[O-]...   \n",
       "7188  qmof-fcb8885              [Ni].c1ccc(cn1)[CH][N][N][CH]c1cccnc1   \n",
       "7189  qmof-d8acb74                  [Cd].[O-]C(=O)c1cccc(c1)C(=O)[O-]   \n",
       "\n",
       "      Band Gap  \n",
       "0     2.201034  \n",
       "1     2.055500  \n",
       "2     3.677709  \n",
       "3     0.959976  \n",
       "4     1.098963  \n",
       "...        ...  \n",
       "7185  2.038372  \n",
       "7186  1.747641  \n",
       "7187  1.665536  \n",
       "7188  1.388292  \n",
       "7189  3.172720  \n",
       "\n",
       "[7190 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.DataFrame(id_smiles_stable)\n",
    "data_df = data_df.dropna()\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MOFTokenizer(\"MOFormer_modded/tokenizer/vocab_full.txt\")\n",
    "config = yaml.load(open(\"MOFormer_modded/config_ft_transformer.yaml\", \"r\"), Loader=yaml.FullLoader)\n",
    "config['dataloader']['randomSeed'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 64, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': './training_results/pretraining', 'trained_with': 'CGCNN', 'log_every_n_steps': 1, 'gpu': 'cuda:0', 'vocab_path': 'MOFormer_modded/tokenizer/vocab_full.txt', 'cuda': True, 'num_workers': 0, 'task': 'classification', 'optim': {'optimizer': 'Adam', 'init_lr': 5e-05, 'weight_decay': '1e-6'}, 'dataloader': {'valid_ratio': 0.15, 'test_ratio': 0.15, 'use_ratio': 1, 'randomSeed': 0}, 'dataset': {'data_name': 'QMOF', 'dataPath': './MOFormer_modded/dataset/core_ch4uptake_highP.csv'}, 'Transformer': {'ntoken': 4021, 'd_model': 512, 'nhead': 8, 'd_hid': 512, 'nlayers': 6, 'dropout': 0.1}}\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_df.to_numpy()\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True, random_state=42)\n",
    "\n",
    "folds = []\n",
    "for train_index, test_index in kf.split(data):\n",
    "    train_fold, test_fold = data[train_index], data[test_index]\n",
    "    folds.append(train_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5752, 3), (5752, 3), (5752, 3), (5752, 3), (5752, 3))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds[0].shape, folds[1].shape, folds[2].shape, folds[3].shape, folds[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MOFormer_modded.transformer import PositionalEncoding\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import math\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout, batch_first=True)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.token_encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        # initrange = 0.1\n",
    "        # self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        nn.init.xavier_normal_(self.token_encoder.weight)\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.token_encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = output[:, 0:1, :] #this was added in by me\n",
    "\n",
    "        return output.squeeze(dim = 1) #this was added in by me\n",
    "        #return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTransformer(nn.Module):\n",
    "    def __init__(self, transformer, input_dim = 512, hidden_dim = 256, output_dim = 1):\n",
    "        super(ClassificationTransformer, self).__init__()\n",
    "        self.transformer = transformer\n",
    "\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Linear(hidden_dim, 2)\n",
    "        ) #nn.Sigmoid was here before\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim = 1)\n",
    "        x = self.classification_head(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class RegressionTransformer(nn.Module):\n",
    "    def __init__(self, model, mlp_hidden_dim=256):\n",
    "        super(RegressionTransformer, self).__init__()\n",
    "        \n",
    "        #initialize model itself\n",
    "        self.model = model\n",
    "\n",
    "        #regression head\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(512, mlp_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        #only updating MLP regression head\n",
    "        #for params in self.model.parameters():\n",
    "        #    params.requires_grad = False\n",
    "                \n",
    "    def forward(self, smiles):\n",
    "        transformer_output = self.model(smiles)\n",
    "\n",
    "        output = self.regression_head(transformer_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded: pos_encoder.pe\n",
      "loaded: transformer_encoder.layers.0.self_attn.in_proj_weight\n",
      "loaded: transformer_encoder.layers.0.self_attn.in_proj_bias\n",
      "loaded: transformer_encoder.layers.0.self_attn.out_proj.weight\n",
      "loaded: transformer_encoder.layers.0.self_attn.out_proj.bias\n",
      "loaded: transformer_encoder.layers.0.linear1.weight\n",
      "loaded: transformer_encoder.layers.0.linear1.bias\n",
      "loaded: transformer_encoder.layers.0.linear2.weight\n",
      "loaded: transformer_encoder.layers.0.linear2.bias\n",
      "loaded: transformer_encoder.layers.0.norm1.weight\n",
      "loaded: transformer_encoder.layers.0.norm1.bias\n",
      "loaded: transformer_encoder.layers.0.norm2.weight\n",
      "loaded: transformer_encoder.layers.0.norm2.bias\n",
      "loaded: transformer_encoder.layers.1.self_attn.in_proj_weight\n",
      "loaded: transformer_encoder.layers.1.self_attn.in_proj_bias\n",
      "loaded: transformer_encoder.layers.1.self_attn.out_proj.weight\n",
      "loaded: transformer_encoder.layers.1.self_attn.out_proj.bias\n",
      "loaded: transformer_encoder.layers.1.linear1.weight\n",
      "loaded: transformer_encoder.layers.1.linear1.bias\n",
      "loaded: transformer_encoder.layers.1.linear2.weight\n",
      "loaded: transformer_encoder.layers.1.linear2.bias\n",
      "loaded: transformer_encoder.layers.1.norm1.weight\n",
      "loaded: transformer_encoder.layers.1.norm1.bias\n",
      "loaded: transformer_encoder.layers.1.norm2.weight\n",
      "loaded: transformer_encoder.layers.1.norm2.bias\n",
      "loaded: transformer_encoder.layers.2.self_attn.in_proj_weight\n",
      "loaded: transformer_encoder.layers.2.self_attn.in_proj_bias\n",
      "loaded: transformer_encoder.layers.2.self_attn.out_proj.weight\n",
      "loaded: transformer_encoder.layers.2.self_attn.out_proj.bias\n",
      "loaded: transformer_encoder.layers.2.linear1.weight\n",
      "loaded: transformer_encoder.layers.2.linear1.bias\n",
      "loaded: transformer_encoder.layers.2.linear2.weight\n",
      "loaded: transformer_encoder.layers.2.linear2.bias\n",
      "loaded: transformer_encoder.layers.2.norm1.weight\n",
      "loaded: transformer_encoder.layers.2.norm1.bias\n",
      "loaded: transformer_encoder.layers.2.norm2.weight\n",
      "loaded: transformer_encoder.layers.2.norm2.bias\n",
      "loaded: transformer_encoder.layers.3.self_attn.in_proj_weight\n",
      "loaded: transformer_encoder.layers.3.self_attn.in_proj_bias\n",
      "loaded: transformer_encoder.layers.3.self_attn.out_proj.weight\n",
      "loaded: transformer_encoder.layers.3.self_attn.out_proj.bias\n",
      "loaded: transformer_encoder.layers.3.linear1.weight\n",
      "loaded: transformer_encoder.layers.3.linear1.bias\n",
      "loaded: transformer_encoder.layers.3.linear2.weight\n",
      "loaded: transformer_encoder.layers.3.linear2.bias\n",
      "loaded: transformer_encoder.layers.3.norm1.weight\n",
      "loaded: transformer_encoder.layers.3.norm1.bias\n",
      "loaded: transformer_encoder.layers.3.norm2.weight\n",
      "loaded: transformer_encoder.layers.3.norm2.bias\n",
      "loaded: transformer_encoder.layers.4.self_attn.in_proj_weight\n",
      "loaded: transformer_encoder.layers.4.self_attn.in_proj_bias\n",
      "loaded: transformer_encoder.layers.4.self_attn.out_proj.weight\n",
      "loaded: transformer_encoder.layers.4.self_attn.out_proj.bias\n",
      "loaded: transformer_encoder.layers.4.linear1.weight\n",
      "loaded: transformer_encoder.layers.4.linear1.bias\n",
      "loaded: transformer_encoder.layers.4.linear2.weight\n",
      "loaded: transformer_encoder.layers.4.linear2.bias\n",
      "loaded: transformer_encoder.layers.4.norm1.weight\n",
      "loaded: transformer_encoder.layers.4.norm1.bias\n",
      "loaded: transformer_encoder.layers.4.norm2.weight\n",
      "loaded: transformer_encoder.layers.4.norm2.bias\n",
      "loaded: transformer_encoder.layers.5.self_attn.in_proj_weight\n",
      "loaded: transformer_encoder.layers.5.self_attn.in_proj_bias\n",
      "loaded: transformer_encoder.layers.5.self_attn.out_proj.weight\n",
      "loaded: transformer_encoder.layers.5.self_attn.out_proj.bias\n",
      "loaded: transformer_encoder.layers.5.linear1.weight\n",
      "loaded: transformer_encoder.layers.5.linear1.bias\n",
      "loaded: transformer_encoder.layers.5.linear2.weight\n",
      "loaded: transformer_encoder.layers.5.linear2.bias\n",
      "loaded: transformer_encoder.layers.5.norm1.weight\n",
      "loaded: transformer_encoder.layers.5.norm1.bias\n",
      "loaded: transformer_encoder.layers.5.norm2.weight\n",
      "loaded: transformer_encoder.layers.5.norm2.bias\n",
      "loaded: token_encoder.weight\n",
      "Loaded pre-trained model with success.\n"
     ]
    }
   ],
   "source": [
    "def _load_pre_trained_weights(model, mode = 'cgcnn'):\n",
    "    \"\"\"\n",
    "    Taken from this repository: https://github.com/zcao0420/MOFormer/blob/main/finetune_transformer.py\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # checkpoints_folder = os.path.join(self.config['fine_tune_from'], 'checkpoints')\n",
    "        #checkpoints_folder = 'SSL/pretrained/transformer'\n",
    "        checkpoints_folder = 'SSL/pretrained/cgcnn'\n",
    "        if mode == 'geometric':\n",
    "            checkpoints_folder = 'SSL/pretrained/geometric'\n",
    "\n",
    "        elif mode == 'cgcnn':\n",
    "            checkpoints_folder = 'SSL/pretrained/transformer'\n",
    "        \n",
    "        else:\n",
    "            checkpoints_folder = 'SSL/pretrained/None'\n",
    "\n",
    "        load_state = torch.load(os.path.join(checkpoints_folder, 'model_t_50.pth'),  map_location=config['gpu']) \n",
    "        model_state = model.state_dict()\n",
    "\n",
    "        for name, param in load_state.items():\n",
    "            if name not in model_state:\n",
    "                print('NOT loaded:', name)\n",
    "                continue\n",
    "            else:\n",
    "                print('loaded:', name)\n",
    "            if isinstance(param, nn.parameter.Parameter):\n",
    "                # backwards compatibility for serialized parameters\n",
    "                param = param.data\n",
    "            model_state[name].copy_(param)\n",
    "        print(\"Loaded pre-trained model with success.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Pre-trained weights not found. Training from scratch.\")\n",
    "\n",
    "    return model\n",
    "\n",
    "transformer_SMILES = Transformer(**config['Transformer'])\n",
    "model_pre = _load_pre_trained_weights(model = transformer_SMILES, mode = 'cgcnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() and config['gpu'] != 'cpu':\n",
    "    device = config['gpu']\n",
    "    torch.cuda.set_device(device)\n",
    "    config['cuda'] = True\n",
    "\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    config['cuda'] = False\n",
    "print(\"Running on:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = ClassificationTransformer(transformer = model_pre)\n",
    "model = RegressionTransformer(model = model_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()\n",
    "\n",
    "optimizer = optim.Adam(model.regression_head.parameters(), lr = 0.01)\n",
    "optimizer_t = optim.Adam(model.model.parameters(), lr = 0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is for evaluation\n",
    "batch_size = 64\n",
    "\n",
    "fifth_db = MOF_ID_Dataset(data = folds[4], tokenizer = tokenizer)\n",
    "\n",
    "fifth_fold = DataLoader(\n",
    "                        fifth_db, batch_size=batch_size, drop_last=True, shuffle=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model number: 0\n",
      "loaded: pos_encoder.pe\n",
      "loaded: transformer_encoder.layers.0.self_attn.in_proj_weight\n",
      "loaded: transformer_encoder.layers.0.self_attn.in_proj_bias\n",
      "loaded: transformer_encoder.layers.0.self_attn.out_proj.weight\n",
      "loaded: transformer_encoder.layers.0.self_attn.out_proj.bias\n",
      "loaded: transformer_encoder.layers.0.linear1.weight\n",
      "loaded: transformer_encoder.layers.0.linear1.bias\n",
      "loaded: transformer_encoder.layers.0.linear2.weight\n",
      "loaded: transformer_encoder.layers.0.linear2.bias\n",
      "loaded: transformer_encoder.layers.0.norm1.weight\n",
      "loaded: transformer_encoder.layers.0.norm1.bias\n",
      "loaded: transformer_encoder.layers.0.norm2.weight\n",
      "loaded: transformer_encoder.layers.0.norm2.bias\n",
      "loaded: transformer_encoder.layers.1.self_attn.in_proj_weight\n",
      "loaded: transformer_encoder.layers.1.self_attn.in_proj_bias\n",
      "loaded: transformer_encoder.layers.1.self_attn.out_proj.weight\n",
      "loaded: transformer_encoder.layers.1.self_attn.out_proj.bias\n",
      "loaded: transformer_encoder.layers.1.linear1.weight\n",
      "loaded: transformer_encoder.layers.1.linear1.bias\n",
      "loaded: transformer_encoder.layers.1.linear2.weight\n",
      "loaded: transformer_encoder.layers.1.linear2.bias\n",
      "loaded: transformer_encoder.layers.1.norm1.weight\n",
      "loaded: transformer_encoder.layers.1.norm1.bias\n",
      "loaded: transformer_encoder.layers.1.norm2.weight\n",
      "loaded: transformer_encoder.layers.1.norm2.bias\n",
      "loaded: transformer_encoder.layers.2.self_attn.in_proj_weight\n",
      "loaded: transformer_encoder.layers.2.self_attn.in_proj_bias\n",
      "loaded: transformer_encoder.layers.2.self_attn.out_proj.weight\n",
      "loaded: transformer_encoder.layers.2.self_attn.out_proj.bias\n",
      "loaded: transformer_encoder.layers.2.linear1.weight\n",
      "loaded: transformer_encoder.layers.2.linear1.bias\n",
      "loaded: transformer_encoder.layers.2.linear2.weight\n",
      "loaded: transformer_encoder.layers.2.linear2.bias\n",
      "loaded: transformer_encoder.layers.2.norm1.weight\n",
      "loaded: transformer_encoder.layers.2.norm1.bias\n",
      "loaded: transformer_encoder.layers.2.norm2.weight\n",
      "loaded: transformer_encoder.layers.2.norm2.bias\n",
      "loaded: transformer_encoder.layers.3.self_attn.in_proj_weight\n",
      "loaded: transformer_encoder.layers.3.self_attn.in_proj_bias\n",
      "loaded: transformer_encoder.layers.3.self_attn.out_proj.weight\n",
      "loaded: transformer_encoder.layers.3.self_attn.out_proj.bias\n",
      "loaded: transformer_encoder.layers.3.linear1.weight\n",
      "loaded: transformer_encoder.layers.3.linear1.bias\n",
      "loaded: transformer_encoder.layers.3.linear2.weight\n",
      "loaded: transformer_encoder.layers.3.linear2.bias\n",
      "loaded: transformer_encoder.layers.3.norm1.weight\n",
      "loaded: transformer_encoder.layers.3.norm1.bias\n",
      "loaded: transformer_encoder.layers.3.norm2.weight\n",
      "loaded: transformer_encoder.layers.3.norm2.bias\n",
      "loaded: transformer_encoder.layers.4.self_attn.in_proj_weight\n",
      "loaded: transformer_encoder.layers.4.self_attn.in_proj_bias\n",
      "loaded: transformer_encoder.layers.4.self_attn.out_proj.weight\n",
      "loaded: transformer_encoder.layers.4.self_attn.out_proj.bias\n",
      "loaded: transformer_encoder.layers.4.linear1.weight\n",
      "loaded: transformer_encoder.layers.4.linear1.bias\n",
      "loaded: transformer_encoder.layers.4.linear2.weight\n",
      "loaded: transformer_encoder.layers.4.linear2.bias\n",
      "loaded: transformer_encoder.layers.4.norm1.weight\n",
      "loaded: transformer_encoder.layers.4.norm1.bias\n",
      "loaded: transformer_encoder.layers.4.norm2.weight\n",
      "loaded: transformer_encoder.layers.4.norm2.bias\n",
      "loaded: transformer_encoder.layers.5.self_attn.in_proj_weight\n",
      "loaded: transformer_encoder.layers.5.self_attn.in_proj_bias\n",
      "loaded: transformer_encoder.layers.5.self_attn.out_proj.weight\n",
      "loaded: transformer_encoder.layers.5.self_attn.out_proj.bias\n",
      "loaded: transformer_encoder.layers.5.linear1.weight\n",
      "loaded: transformer_encoder.layers.5.linear1.bias\n",
      "loaded: transformer_encoder.layers.5.linear2.weight\n",
      "loaded: transformer_encoder.layers.5.linear2.bias\n",
      "loaded: transformer_encoder.layers.5.norm1.weight\n",
      "loaded: transformer_encoder.layers.5.norm1.bias\n",
      "loaded: transformer_encoder.layers.5.norm2.weight\n",
      "loaded: transformer_encoder.layers.5.norm2.bias\n",
      "loaded: token_encoder.weight\n",
      "Loaded pre-trained model with success.\n",
      "Epoch: 1, Batch: 88, Loss: 0.7765457543094506, Val Loss: 0.6476506808500612, SRCC_test = 0.7125256487749303\n",
      "Epoch: 2, Batch: 88, Loss: 0.5378677439823579, Val Loss: 0.4724719132600206, SRCC_test = 0.7848504285874421\n",
      "Epoch: 3, Batch: 88, Loss: 0.48995275946145644, Val Loss: 0.4803280984417776, SRCC_test = 0.7914484760772397\n",
      "Epoch: 4, Batch: 88, Loss: 0.47078296609139175, Val Loss: 0.4583114566427938, SRCC_test = 0.8029277756526552\n",
      "Epoch: 5, Batch: 88, Loss: 0.4565650405508749, Val Loss: 0.43713313809941323, SRCC_test = 0.8058974913469679\n",
      "Epoch: 6, Batch: 88, Loss: 0.45452066686715975, Val Loss: 0.4443687026420336, SRCC_test = 0.8115109977911141\n",
      "Epoch: 7, Batch: 88, Loss: 0.4412939682435454, Val Loss: 0.4385991163468093, SRCC_test = 0.81206147691494\n",
      "Epoch: 8, Batch: 88, Loss: 0.4252111982093768, Val Loss: 0.4479984570755048, SRCC_test = 0.8307791912469801\n",
      "Epoch: 9, Batch: 88, Loss: 0.4200235839640157, Val Loss: 0.3943482834971353, SRCC_test = 0.8329102450556347\n",
      "Epoch: 10, Batch: 88, Loss: 0.41693787661831033, Val Loss: 0.4090927179609792, SRCC_test = 0.8346067569258443\n",
      "Epoch: 11, Batch: 88, Loss: 0.4094568504376358, Val Loss: 0.40148066000991994, SRCC_test = 0.8426833820666151\n",
      "Epoch: 12, Batch: 88, Loss: 0.40601583511641853, Val Loss: 0.4036883882592233, SRCC_test = 0.8458395140032644\n",
      "Epoch: 13, Batch: 88, Loss: 0.3937223801452122, Val Loss: 0.3810488685463252, SRCC_test = 0.8450645425133446\n",
      "Epoch: 14, Batch: 88, Loss: 0.39195917228634436, Val Loss: 0.3776126212618324, SRCC_test = 0.8437827548643704\n",
      "Epoch: 15, Batch: 88, Loss: 0.39126685127783356, Val Loss: 0.4125941295302316, SRCC_test = 0.8390338140609044\n",
      "Epoch: 16, Batch: 88, Loss: 0.4042039493496498, Val Loss: 0.41428528776329554, SRCC_test = 0.848252037428578\n",
      "Epoch: 17, Batch: 88, Loss: 0.38105487756514816, Val Loss: 0.36064016668314347, SRCC_test = 0.8633076485896086\n",
      "Epoch: 18, Batch: 88, Loss: 0.3766399256968766, Val Loss: 0.37279873115293094, SRCC_test = 0.8590230492657229\n",
      "Epoch: 19, Batch: 88, Loss: 0.36957856610919654, Val Loss: 0.3511554099870532, SRCC_test = 0.8696814319772339\n",
      "Epoch: 20, Batch: 88, Loss: 0.36760014931807355, Val Loss: 0.3674903217326389, SRCC_test = 0.8583112853205732\n",
      "Training model number: 1\n",
      "loaded: pos_encoder.pe\n",
      "loaded: transformer_encoder.layers.0.self_attn.in_proj_weight\n",
      "loaded: transformer_encoder.layers.0.self_attn.in_proj_bias\n",
      "loaded: transformer_encoder.layers.0.self_attn.out_proj.weight\n",
      "loaded: transformer_encoder.layers.0.self_attn.out_proj.bias\n",
      "loaded: transformer_encoder.layers.0.linear1.weight\n",
      "loaded: transformer_encoder.layers.0.linear1.bias\n",
      "loaded: transformer_encoder.layers.0.linear2.weight\n",
      "loaded: transformer_encoder.layers.0.linear2.bias\n",
      "loaded: transformer_encoder.layers.0.norm1.weight\n",
      "loaded: transformer_encoder.layers.0.norm1.bias\n",
      "loaded: transformer_encoder.layers.0.norm2.weight\n",
      "loaded: transformer_encoder.layers.0.norm2.bias\n",
      "loaded: transformer_encoder.layers.1.self_attn.in_proj_weight\n",
      "loaded: transformer_encoder.layers.1.self_attn.in_proj_bias\n",
      "loaded: transformer_encoder.layers.1.self_attn.out_proj.weight\n",
      "loaded: transformer_encoder.layers.1.self_attn.out_proj.bias\n",
      "loaded: transformer_encoder.layers.1.linear1.weight\n",
      "loaded: transformer_encoder.layers.1.linear1.bias\n",
      "loaded: transformer_encoder.layers.1.linear2.weight\n",
      "loaded: transformer_encoder.layers.1.linear2.bias\n",
      "loaded: transformer_encoder.layers.1.norm1.weight\n",
      "loaded: transformer_encoder.layers.1.norm1.bias\n",
      "loaded: transformer_encoder.layers.1.norm2.weight\n",
      "loaded: transformer_encoder.layers.1.norm2.bias\n",
      "loaded: transformer_encoder.layers.2.self_attn.in_proj_weight\n",
      "loaded: transformer_encoder.layers.2.self_attn.in_proj_bias\n",
      "loaded: transformer_encoder.layers.2.self_attn.out_proj.weight\n",
      "loaded: transformer_encoder.layers.2.self_attn.out_proj.bias\n",
      "loaded: transformer_encoder.layers.2.linear1.weight\n",
      "loaded: transformer_encoder.layers.2.linear1.bias\n",
      "loaded: transformer_encoder.layers.2.linear2.weight\n",
      "loaded: transformer_encoder.layers.2.linear2.bias\n",
      "loaded: transformer_encoder.layers.2.norm1.weight\n",
      "loaded: transformer_encoder.layers.2.norm1.bias\n",
      "loaded: transformer_encoder.layers.2.norm2.weight\n",
      "loaded: transformer_encoder.layers.2.norm2.bias\n",
      "loaded: transformer_encoder.layers.3.self_attn.in_proj_weight\n",
      "loaded: transformer_encoder.layers.3.self_attn.in_proj_bias\n",
      "loaded: transformer_encoder.layers.3.self_attn.out_proj.weight\n",
      "loaded: transformer_encoder.layers.3.self_attn.out_proj.bias\n",
      "loaded: transformer_encoder.layers.3.linear1.weight\n",
      "loaded: transformer_encoder.layers.3.linear1.bias\n",
      "loaded: transformer_encoder.layers.3.linear2.weight\n",
      "loaded: transformer_encoder.layers.3.linear2.bias\n",
      "loaded: transformer_encoder.layers.3.norm1.weight\n",
      "loaded: transformer_encoder.layers.3.norm1.bias\n",
      "loaded: transformer_encoder.layers.3.norm2.weight\n",
      "loaded: transformer_encoder.layers.3.norm2.bias\n",
      "loaded: transformer_encoder.layers.4.self_attn.in_proj_weight\n",
      "loaded: transformer_encoder.layers.4.self_attn.in_proj_bias\n",
      "loaded: transformer_encoder.layers.4.self_attn.out_proj.weight\n",
      "loaded: transformer_encoder.layers.4.self_attn.out_proj.bias\n",
      "loaded: transformer_encoder.layers.4.linear1.weight\n",
      "loaded: transformer_encoder.layers.4.linear1.bias\n",
      "loaded: transformer_encoder.layers.4.linear2.weight\n",
      "loaded: transformer_encoder.layers.4.linear2.bias\n",
      "loaded: transformer_encoder.layers.4.norm1.weight\n",
      "loaded: transformer_encoder.layers.4.norm1.bias\n",
      "loaded: transformer_encoder.layers.4.norm2.weight\n",
      "loaded: transformer_encoder.layers.4.norm2.bias\n",
      "loaded: transformer_encoder.layers.5.self_attn.in_proj_weight\n",
      "loaded: transformer_encoder.layers.5.self_attn.in_proj_bias\n",
      "loaded: transformer_encoder.layers.5.self_attn.out_proj.weight\n",
      "loaded: transformer_encoder.layers.5.self_attn.out_proj.bias\n",
      "loaded: transformer_encoder.layers.5.linear1.weight\n",
      "loaded: transformer_encoder.layers.5.linear1.bias\n",
      "loaded: transformer_encoder.layers.5.linear2.weight\n",
      "loaded: transformer_encoder.layers.5.linear2.bias\n",
      "loaded: transformer_encoder.layers.5.norm1.weight\n",
      "loaded: transformer_encoder.layers.5.norm1.bias\n",
      "loaded: transformer_encoder.layers.5.norm2.weight\n",
      "loaded: transformer_encoder.layers.5.norm2.bias\n",
      "loaded: token_encoder.weight\n",
      "Loaded pre-trained model with success.\n",
      "Epoch: 1, Batch: 88, Loss: 0.7903886732090725, Val Loss: 0.5210607329781136, SRCC_test = 0.7253769384575538\n",
      "Epoch: 2, Batch: 88, Loss: 0.5234230110484562, Val Loss: 0.5029943538515755, SRCC_test = 0.7855207448045517\n",
      "Epoch: 3, Batch: 88, Loss: 0.4802664454733388, Val Loss: 0.4460369934526722, SRCC_test = 0.8024265035703527\n",
      "Epoch: 4, Batch: 88, Loss: 0.4680589162901546, Val Loss: 0.4464198588655236, SRCC_test = 0.8091588502569906\n",
      "Epoch: 5, Batch: 88, Loss: 0.4452229507183761, Val Loss: 0.4282592036081164, SRCC_test = 0.810882090198011\n",
      "Epoch: 6, Batch: 88, Loss: 0.4331434844584947, Val Loss: 0.41138459758812124, SRCC_test = 0.8213200504363556\n",
      "Epoch: 7, Batch: 88, Loss: 0.4223711112242066, Val Loss: 0.41904687211754615, SRCC_test = 0.8240417441012589\n",
      "Epoch: 8, Batch: 88, Loss: 0.42140524072593516, Val Loss: 0.4003021710374382, SRCC_test = 0.8331744427860894\n",
      "Epoch: 9, Batch: 88, Loss: 0.4165554130345248, Val Loss: 0.4010747741447406, SRCC_test = 0.8320745621785386\n",
      "Epoch: 10, Batch: 88, Loss: 0.4113652448305923, Val Loss: 0.3996001431781254, SRCC_test = 0.8361064115517922\n",
      "Epoch: 11, Batch: 88, Loss: 0.4037154142776232, Val Loss: 0.38877153731464, SRCC_test = 0.8392201555369435\n",
      "Epoch: 12, Batch: 88, Loss: 0.4019278465362077, Val Loss: 0.3885620105802343, SRCC_test = 0.8432430745719669\n",
      "Epoch: 13, Batch: 88, Loss: 0.40770359052700944, Val Loss: 0.4177705638864067, SRCC_test = 0.8502516068774738\n",
      "Epoch: 14, Batch: 88, Loss: 0.38975463288553647, Val Loss: 0.3857063122009963, SRCC_test = 0.8448273730118884\n",
      "Epoch: 15, Batch: 88, Loss: 0.3843012706617291, Val Loss: 0.381581297081508, SRCC_test = 0.8503136453901743\n",
      "Epoch: 16, Batch: 88, Loss: 0.3881502526529719, Val Loss: 0.3907491410716196, SRCC_test = 0.8507754578166461\n",
      "Epoch: 17, Batch: 88, Loss: 0.3798513683710205, Val Loss: 0.3657281080658516, SRCC_test = 0.8586265380361995\n",
      "Epoch: 18, Batch: 88, Loss: 0.3687269235594889, Val Loss: 0.3850866137595659, SRCC_test = 0.858412592862487\n",
      "Epoch: 19, Batch: 88, Loss: 0.3694416977716296, Val Loss: 0.42545792862270654, SRCC_test = 0.8578773871849238\n",
      "Epoch: 20, Batch: 88, Loss: 0.36519211999486, Val Loss: 0.36544092719474536, SRCC_test = 0.8620169536536865\n",
      "Training model number: 2\n",
      "loaded: pos_encoder.pe\n",
      "loaded: transformer_encoder.layers.0.self_attn.in_proj_weight\n",
      "loaded: transformer_encoder.layers.0.self_attn.in_proj_bias\n",
      "loaded: transformer_encoder.layers.0.self_attn.out_proj.weight\n",
      "loaded: transformer_encoder.layers.0.self_attn.out_proj.bias\n",
      "loaded: transformer_encoder.layers.0.linear1.weight\n",
      "loaded: transformer_encoder.layers.0.linear1.bias\n",
      "loaded: transformer_encoder.layers.0.linear2.weight\n",
      "loaded: transformer_encoder.layers.0.linear2.bias\n",
      "loaded: transformer_encoder.layers.0.norm1.weight\n",
      "loaded: transformer_encoder.layers.0.norm1.bias\n",
      "loaded: transformer_encoder.layers.0.norm2.weight\n",
      "loaded: transformer_encoder.layers.0.norm2.bias\n",
      "loaded: transformer_encoder.layers.1.self_attn.in_proj_weight\n",
      "loaded: transformer_encoder.layers.1.self_attn.in_proj_bias\n",
      "loaded: transformer_encoder.layers.1.self_attn.out_proj.weight\n",
      "loaded: transformer_encoder.layers.1.self_attn.out_proj.bias\n",
      "loaded: transformer_encoder.layers.1.linear1.weight\n",
      "loaded: transformer_encoder.layers.1.linear1.bias\n",
      "loaded: transformer_encoder.layers.1.linear2.weight\n",
      "loaded: transformer_encoder.layers.1.linear2.bias\n",
      "loaded: transformer_encoder.layers.1.norm1.weight\n",
      "loaded: transformer_encoder.layers.1.norm1.bias\n",
      "loaded: transformer_encoder.layers.1.norm2.weight\n",
      "loaded: transformer_encoder.layers.1.norm2.bias\n",
      "loaded: transformer_encoder.layers.2.self_attn.in_proj_weight\n",
      "loaded: transformer_encoder.layers.2.self_attn.in_proj_bias\n",
      "loaded: transformer_encoder.layers.2.self_attn.out_proj.weight\n",
      "loaded: transformer_encoder.layers.2.self_attn.out_proj.bias\n",
      "loaded: transformer_encoder.layers.2.linear1.weight\n",
      "loaded: transformer_encoder.layers.2.linear1.bias\n",
      "loaded: transformer_encoder.layers.2.linear2.weight\n",
      "loaded: transformer_encoder.layers.2.linear2.bias\n",
      "loaded: transformer_encoder.layers.2.norm1.weight\n",
      "loaded: transformer_encoder.layers.2.norm1.bias\n",
      "loaded: transformer_encoder.layers.2.norm2.weight\n",
      "loaded: transformer_encoder.layers.2.norm2.bias\n",
      "loaded: transformer_encoder.layers.3.self_attn.in_proj_weight\n",
      "loaded: transformer_encoder.layers.3.self_attn.in_proj_bias\n",
      "loaded: transformer_encoder.layers.3.self_attn.out_proj.weight\n",
      "loaded: transformer_encoder.layers.3.self_attn.out_proj.bias\n",
      "loaded: transformer_encoder.layers.3.linear1.weight\n",
      "loaded: transformer_encoder.layers.3.linear1.bias\n",
      "loaded: transformer_encoder.layers.3.linear2.weight\n",
      "loaded: transformer_encoder.layers.3.linear2.bias\n",
      "loaded: transformer_encoder.layers.3.norm1.weight\n",
      "loaded: transformer_encoder.layers.3.norm1.bias\n",
      "loaded: transformer_encoder.layers.3.norm2.weight\n",
      "loaded: transformer_encoder.layers.3.norm2.bias\n",
      "loaded: transformer_encoder.layers.4.self_attn.in_proj_weight\n",
      "loaded: transformer_encoder.layers.4.self_attn.in_proj_bias\n",
      "loaded: transformer_encoder.layers.4.self_attn.out_proj.weight\n",
      "loaded: transformer_encoder.layers.4.self_attn.out_proj.bias\n",
      "loaded: transformer_encoder.layers.4.linear1.weight\n",
      "loaded: transformer_encoder.layers.4.linear1.bias\n",
      "loaded: transformer_encoder.layers.4.linear2.weight\n",
      "loaded: transformer_encoder.layers.4.linear2.bias\n",
      "loaded: transformer_encoder.layers.4.norm1.weight\n",
      "loaded: transformer_encoder.layers.4.norm1.bias\n",
      "loaded: transformer_encoder.layers.4.norm2.weight\n",
      "loaded: transformer_encoder.layers.4.norm2.bias\n",
      "loaded: transformer_encoder.layers.5.self_attn.in_proj_weight\n",
      "loaded: transformer_encoder.layers.5.self_attn.in_proj_bias\n",
      "loaded: transformer_encoder.layers.5.self_attn.out_proj.weight\n",
      "loaded: transformer_encoder.layers.5.self_attn.out_proj.bias\n",
      "loaded: transformer_encoder.layers.5.linear1.weight\n",
      "loaded: transformer_encoder.layers.5.linear1.bias\n",
      "loaded: transformer_encoder.layers.5.linear2.weight\n",
      "loaded: transformer_encoder.layers.5.linear2.bias\n",
      "loaded: transformer_encoder.layers.5.norm1.weight\n",
      "loaded: transformer_encoder.layers.5.norm1.bias\n",
      "loaded: transformer_encoder.layers.5.norm2.weight\n",
      "loaded: transformer_encoder.layers.5.norm2.bias\n",
      "loaded: token_encoder.weight\n",
      "Loaded pre-trained model with success.\n",
      "Epoch: 1, Batch: 88, Loss: 0.8312134558565161, Val Loss: 0.5303220109323438, SRCC_test = 0.7360816885313314\n",
      "Epoch: 2, Batch: 88, Loss: 0.5309147697486235, Val Loss: 0.5005243775549899, SRCC_test = 0.7753912781704162\n",
      "Epoch: 3, Batch: 88, Loss: 0.485515878776486, Val Loss: 0.4438939241880781, SRCC_test = 0.7987340907651244\n",
      "Epoch: 4, Batch: 88, Loss: 0.46553569864690975, Val Loss: 0.4601940827423267, SRCC_test = 0.8079760998611057\n",
      "Epoch: 5, Batch: 88, Loss: 0.45283133394262765, Val Loss: 0.42632623435406203, SRCC_test = 0.814141055260712\n",
      "Epoch: 6, Batch: 88, Loss: 0.4343104837985521, Val Loss: 0.4276579591665375, SRCC_test = 0.8188387692956693\n",
      "Epoch: 7, Batch: 88, Loss: 0.43699896302116054, Val Loss: 0.46866539154159886, SRCC_test = 0.8136432808002282\n",
      "Epoch: 8, Batch: 88, Loss: 0.43063407165280887, Val Loss: 0.5033490470286166, SRCC_test = 0.8252371377247845\n",
      "Epoch: 9, Batch: 88, Loss: 0.42209232087885395, Val Loss: 0.4128764603245124, SRCC_test = 0.8273114817877664\n",
      "Epoch: 10, Batch: 88, Loss: 0.41119306074099593, Val Loss: 0.41385251551531677, SRCC_test = 0.8354167590841517\n",
      "Epoch: 11, Batch: 88, Loss: 0.4147927399431722, Val Loss: 0.4064211718152078, SRCC_test = 0.8479020266892929\n",
      "Epoch: 12, Batch: 88, Loss: 0.40024039718542204, Val Loss: 0.5214680128552941, SRCC_test = 0.8472032184779116\n",
      "Epoch: 13, Batch: 88, Loss: 0.414443579617511, Val Loss: 0.41336279772640616, SRCC_test = 0.8462752589905211\n",
      "Epoch: 14, Batch: 88, Loss: 0.3884197363023008, Val Loss: 0.3793390423394321, SRCC_test = 0.8533845799335733\n",
      "Epoch: 15, Batch: 88, Loss: 0.3840680999702282, Val Loss: 0.3956317228547643, SRCC_test = 0.8508861243003765\n",
      "Epoch: 16, Batch: 88, Loss: 0.3848819290654043, Val Loss: 0.3780557282185287, SRCC_test = 0.854148239212852\n",
      "Epoch: 17, Batch: 88, Loss: 0.3728442861792747, Val Loss: 0.37677900911716933, SRCC_test = 0.858006278076609\n",
      "Epoch: 18, Batch: 88, Loss: 0.37754013833035244, Val Loss: 0.3661774416987816, SRCC_test = 0.8629891495574105\n",
      "Epoch: 19, Batch: 88, Loss: 0.3838907590742861, Val Loss: 0.3742131908957878, SRCC_test = 0.8626445151259212\n",
      "Epoch: 20, Batch: 88, Loss: 0.36267270064086055, Val Loss: 0.3544798523522495, SRCC_test = 0.864377748725632\n",
      "Training model number: 3\n",
      "loaded: pos_encoder.pe\n",
      "loaded: transformer_encoder.layers.0.self_attn.in_proj_weight\n",
      "loaded: transformer_encoder.layers.0.self_attn.in_proj_bias\n",
      "loaded: transformer_encoder.layers.0.self_attn.out_proj.weight\n",
      "loaded: transformer_encoder.layers.0.self_attn.out_proj.bias\n",
      "loaded: transformer_encoder.layers.0.linear1.weight\n",
      "loaded: transformer_encoder.layers.0.linear1.bias\n",
      "loaded: transformer_encoder.layers.0.linear2.weight\n",
      "loaded: transformer_encoder.layers.0.linear2.bias\n",
      "loaded: transformer_encoder.layers.0.norm1.weight\n",
      "loaded: transformer_encoder.layers.0.norm1.bias\n",
      "loaded: transformer_encoder.layers.0.norm2.weight\n",
      "loaded: transformer_encoder.layers.0.norm2.bias\n",
      "loaded: transformer_encoder.layers.1.self_attn.in_proj_weight\n",
      "loaded: transformer_encoder.layers.1.self_attn.in_proj_bias\n",
      "loaded: transformer_encoder.layers.1.self_attn.out_proj.weight\n",
      "loaded: transformer_encoder.layers.1.self_attn.out_proj.bias\n",
      "loaded: transformer_encoder.layers.1.linear1.weight\n",
      "loaded: transformer_encoder.layers.1.linear1.bias\n",
      "loaded: transformer_encoder.layers.1.linear2.weight\n",
      "loaded: transformer_encoder.layers.1.linear2.bias\n",
      "loaded: transformer_encoder.layers.1.norm1.weight\n",
      "loaded: transformer_encoder.layers.1.norm1.bias\n",
      "loaded: transformer_encoder.layers.1.norm2.weight\n",
      "loaded: transformer_encoder.layers.1.norm2.bias\n",
      "loaded: transformer_encoder.layers.2.self_attn.in_proj_weight\n",
      "loaded: transformer_encoder.layers.2.self_attn.in_proj_bias\n",
      "loaded: transformer_encoder.layers.2.self_attn.out_proj.weight\n",
      "loaded: transformer_encoder.layers.2.self_attn.out_proj.bias\n",
      "loaded: transformer_encoder.layers.2.linear1.weight\n",
      "loaded: transformer_encoder.layers.2.linear1.bias\n",
      "loaded: transformer_encoder.layers.2.linear2.weight\n",
      "loaded: transformer_encoder.layers.2.linear2.bias\n",
      "loaded: transformer_encoder.layers.2.norm1.weight\n",
      "loaded: transformer_encoder.layers.2.norm1.bias\n",
      "loaded: transformer_encoder.layers.2.norm2.weight\n",
      "loaded: transformer_encoder.layers.2.norm2.bias\n",
      "loaded: transformer_encoder.layers.3.self_attn.in_proj_weight\n",
      "loaded: transformer_encoder.layers.3.self_attn.in_proj_bias\n",
      "loaded: transformer_encoder.layers.3.self_attn.out_proj.weight\n",
      "loaded: transformer_encoder.layers.3.self_attn.out_proj.bias\n",
      "loaded: transformer_encoder.layers.3.linear1.weight\n",
      "loaded: transformer_encoder.layers.3.linear1.bias\n",
      "loaded: transformer_encoder.layers.3.linear2.weight\n",
      "loaded: transformer_encoder.layers.3.linear2.bias\n",
      "loaded: transformer_encoder.layers.3.norm1.weight\n",
      "loaded: transformer_encoder.layers.3.norm1.bias\n",
      "loaded: transformer_encoder.layers.3.norm2.weight\n",
      "loaded: transformer_encoder.layers.3.norm2.bias\n",
      "loaded: transformer_encoder.layers.4.self_attn.in_proj_weight\n",
      "loaded: transformer_encoder.layers.4.self_attn.in_proj_bias\n",
      "loaded: transformer_encoder.layers.4.self_attn.out_proj.weight\n",
      "loaded: transformer_encoder.layers.4.self_attn.out_proj.bias\n",
      "loaded: transformer_encoder.layers.4.linear1.weight\n",
      "loaded: transformer_encoder.layers.4.linear1.bias\n",
      "loaded: transformer_encoder.layers.4.linear2.weight\n",
      "loaded: transformer_encoder.layers.4.linear2.bias\n",
      "loaded: transformer_encoder.layers.4.norm1.weight\n",
      "loaded: transformer_encoder.layers.4.norm1.bias\n",
      "loaded: transformer_encoder.layers.4.norm2.weight\n",
      "loaded: transformer_encoder.layers.4.norm2.bias\n",
      "loaded: transformer_encoder.layers.5.self_attn.in_proj_weight\n",
      "loaded: transformer_encoder.layers.5.self_attn.in_proj_bias\n",
      "loaded: transformer_encoder.layers.5.self_attn.out_proj.weight\n",
      "loaded: transformer_encoder.layers.5.self_attn.out_proj.bias\n",
      "loaded: transformer_encoder.layers.5.linear1.weight\n",
      "loaded: transformer_encoder.layers.5.linear1.bias\n",
      "loaded: transformer_encoder.layers.5.linear2.weight\n",
      "loaded: transformer_encoder.layers.5.linear2.bias\n",
      "loaded: transformer_encoder.layers.5.norm1.weight\n",
      "loaded: transformer_encoder.layers.5.norm1.bias\n",
      "loaded: transformer_encoder.layers.5.norm2.weight\n",
      "loaded: transformer_encoder.layers.5.norm2.bias\n",
      "loaded: token_encoder.weight\n",
      "Loaded pre-trained model with success.\n",
      "Epoch: 1, Batch: 88, Loss: 0.8060125306750952, Val Loss: 0.554384232906813, SRCC_test = 0.7047191999185977\n",
      "Epoch: 2, Batch: 88, Loss: 0.5193611611141248, Val Loss: 0.4864117087942831, SRCC_test = 0.7829435512175619\n",
      "Epoch: 3, Batch: 88, Loss: 0.48500215270546043, Val Loss: 0.45375366358274827, SRCC_test = 0.7960339842952386\n",
      "Epoch: 4, Batch: 88, Loss: 0.4649530699413814, Val Loss: 0.4465097366424089, SRCC_test = 0.7988377253606391\n",
      "Epoch: 5, Batch: 88, Loss: 0.4431923514001825, Val Loss: 0.4236817647901814, SRCC_test = 0.8164781114784679\n",
      "Epoch: 6, Batch: 88, Loss: 0.43671649058213396, Val Loss: 0.4154698209146435, SRCC_test = 0.823077994533102\n",
      "Epoch: 7, Batch: 88, Loss: 0.4370775845613373, Val Loss: 0.4149737274378873, SRCC_test = 0.8206197319813049\n",
      "Epoch: 8, Batch: 88, Loss: 0.4207718174779013, Val Loss: 0.4055455243319608, SRCC_test = 0.8336189627649595\n",
      "Epoch: 9, Batch: 88, Loss: 0.41157682256752187, Val Loss: 0.39989007356461514, SRCC_test = 0.830982692211387\n",
      "Epoch: 10, Batch: 88, Loss: 0.41207469815618536, Val Loss: 0.44740304317367213, SRCC_test = 0.8344630221618822\n",
      "Epoch: 11, Batch: 88, Loss: 0.391894988799363, Val Loss: 0.37641213817542857, SRCC_test = 0.8486375850891266\n",
      "Epoch: 12, Batch: 88, Loss: 0.39514683806494383, Val Loss: 0.41248781393083295, SRCC_test = 0.8441748500100745\n",
      "Epoch: 13, Batch: 88, Loss: 0.39195691233270624, Val Loss: 0.3949226305056154, SRCC_test = 0.850936240205383\n",
      "Epoch: 14, Batch: 88, Loss: 0.395950854159473, Val Loss: 0.3803842891468091, SRCC_test = 0.8466103576299701\n",
      "Epoch: 15, Batch: 88, Loss: 0.3755695451511426, Val Loss: 0.37146564146106165, SRCC_test = 0.8497592888913283\n",
      "Epoch: 16, Batch: 88, Loss: 0.3757856146673138, Val Loss: 0.3691350089365177, SRCC_test = 0.8553610473317534\n",
      "Epoch: 17, Batch: 88, Loss: 0.36790728368116227, Val Loss: 0.3865971719281057, SRCC_test = 0.8504909551822574\n",
      "Epoch: 18, Batch: 88, Loss: 0.36380519123559585, Val Loss: 0.3545626723364498, SRCC_test = 0.8615880220594935\n",
      "Epoch: 19, Batch: 88, Loss: 0.3710129036662284, Val Loss: 0.3651587323526318, SRCC_test = 0.8584266902490769\n",
      "Epoch: 20, Batch: 88, Loss: 0.3576385144437297, Val Loss: 0.3543343021628562, SRCC_test = 0.8681481648725038\n"
     ]
    }
   ],
   "source": [
    "n_iter = 0\n",
    "valid_n_iter = 0\n",
    "best_valid_loss = np.inf\n",
    "best_valid_mae = np.inf\n",
    "best_valid_roc_auc = 0\n",
    "best_srcc_valid = 0\n",
    "\n",
    "num_epoch = 20\n",
    "\n",
    "for i in range(4):\n",
    "    print(f\"Training model number: {i}\")\n",
    "    loss_history, val_history, srcc_val_history = [], [], []\n",
    "\n",
    "    transformer_SMILES = Transformer(**config['Transformer'])\n",
    "    model_pre = _load_pre_trained_weights(model = transformer_SMILES, mode = 'cgcnn')\n",
    "    model = RegressionTransformer(model = model_pre)\n",
    "\n",
    "    criterion = nn.L1Loss()\n",
    "\n",
    "    optimizer = optim.Adam(model.regression_head.parameters(), lr = 0.01)\n",
    "    optimizer_t = optim.Adam(model.model.parameters(), lr = 0.00005)\n",
    "\n",
    "    model.train()\n",
    "    some_fold = MOF_ID_Dataset(data = folds[i], tokenizer = tokenizer)\n",
    "\n",
    "    n_fold = DataLoader(\n",
    "                        some_fold, batch_size=batch_size, drop_last=True, shuffle=True\n",
    "                    )\n",
    "    \n",
    "    for epoch_counter in range(num_epoch):\n",
    "        loss_temp = []\n",
    "        for bn, (input1, target) in enumerate(n_fold):\n",
    "            if config['cuda']:\n",
    "                input_var_1 = input1.to(device)\n",
    "                #input_var_2 = input2.to(device)\n",
    "            else:\n",
    "                input_var_1 = input1.to(device)\n",
    "                #input_var_2 = input2.to(device)\n",
    "            \n",
    "            if config['cuda']:\n",
    "                #target_var = Variable(target_normed.to(device, non_blocking=True)) #experimenting with normalization vs non-norm\n",
    "                target_var = Variable(target.to(device, non_blocking=True))\n",
    "            else:\n",
    "                #target_var = Variable(target_normed)\n",
    "                target_var = Variable(target)\n",
    "            \n",
    "            if config['cuda']:\n",
    "                model = model.to(device)\n",
    "\n",
    "            target_var = target_var.reshape(-1, 1)\n",
    "\n",
    "            # compute output\n",
    "            output = model(input_var_1)\n",
    "            output = output.reshape(-1, 1)\n",
    "\n",
    "            loss = criterion(output, target_var)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            optimizer_t.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer_t.step()\n",
    "            n_iter += 1\n",
    "\n",
    "            loss_temp.append(loss.item())\n",
    "        \n",
    "        loss_history.append(np.mean(loss_temp))\n",
    "\n",
    "        val_temp = []\n",
    "        srcc_val_temp = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for bn, (input1, target) in enumerate(fifth_fold):\n",
    "                if config['cuda']:\n",
    "                    input_var_1 = input1.to(device)\n",
    "                    #input_var_2 = input2.to(device)\n",
    "                else:\n",
    "                    input_var_1 = input1.to(device)\n",
    "                    #input_var_2 = input2.to(device)\n",
    "                \n",
    "                \n",
    "                if config['cuda']:\n",
    "                    #target_var = Variable(target_normed.to(device, non_blocking=True))\n",
    "                    target_var = Variable(target.to(device, non_blocking=True))\n",
    "                else:\n",
    "                    #target_var = Variable(target_normed)\n",
    "                    target_var = Variable(target)\n",
    "\n",
    "                target_var = target_var.reshape(-1, 1)\n",
    "                # compute output\n",
    "                output = model(input_var_1)\n",
    "                output = output.reshape(-1, 1)\n",
    "\n",
    "                loss_val = criterion(output, target_var)\n",
    "                val_temp.append(loss_val.item())\n",
    "                srcc_val_temp.append(scipy.stats.spearmanr(output.cpu().numpy(), target_var.cpu().numpy())[0])\n",
    "        \n",
    "        srcc_val_history.append(np.mean(srcc_val_temp))\n",
    "\n",
    "        \n",
    "        val_history.append(np.mean(val_temp))\n",
    "\n",
    "        if epoch_counter % config['log_every_n_steps'] == 0:\n",
    "            print(f'Epoch: {epoch_counter+1}, Batch: {bn}, Loss: {loss_history[-1]}, Val Loss: {val_history[-1]}, SRCC_test = {srcc_val_history[-1]}')\n",
    "        \n",
    "    torch.save(model.state_dict(), f'model_ft_bandgap_{i}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMILES_to_property(smiles, model, tokenizer, device):\n",
    "    token = np.array([tokenizer.encode(smiles, max_length=512, truncation=True,padding='max_length')])\n",
    "    token = torch.from_numpy(np.asarray(token))\n",
    "\n",
    "    token = token.to(device)\n",
    "    return model(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.3134]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SMILES_to_property('[Zn]12.OC(=O)C1=CC=C(C=C1)C(O2)=O', model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Learning curve')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/SrBM8AAAACXBIWXMAAA9hAAAPYQGoP6dpAABidklEQVR4nO3dd3gVVeLG8e9NL6QQ0iEQepMmJTQLSlUR1BXEQhNcEdbCuqvoCir7EyvLIioWii67CiJWFAQEla4U6R3pSWhphLR75/fHkAuREFJuSXk/z3OfzJ17ZuZMhmtez5w5x2IYhoGIiIhIJeHh7gqIiIiIOJLCjYiIiFQqCjciIiJSqSjciIiISKWicCMiIiKVisKNiIiIVCoKNyIiIlKpKNyIiIhIpaJwIyIiIpWKwo2IlEvx8fEMHTrU3dUQkQpI4UakEps9ezYWi4Vff/3V3VUREXEZL3dXQESkMLt378bDQ///JSIlp/9yiIjT5eXlkZOTU6JtfH198fb2dlKN3OvcuXPuroJIpaZwIyIcO3aM4cOHExUVha+vL82bN2fmzJkFyuTk5DB+/Hjatm1LSEgIgYGBXHfddSxfvrxAud9//x2LxcLrr7/OlClTqF+/Pr6+vuzYsYPnn38ei8XCvn37GDp0KKGhoYSEhDBs2DAyMzML7OePfW7yb7GtWrWKsWPHEhERQWBgIHfccQcnT54ssK3NZuP5558nNjaWgIAAunXrxo4dO4rdj8dms/Hvf/+bFi1a4OfnR0REBL1797bf3ss/x9mzZ1+2rcVi4fnnn7e/zz/nHTt2cO+991K9enW6du3K66+/jsVi4dChQ5ftY9y4cfj4+HD27Fn7unXr1tG7d29CQkIICAjghhtuYNWqVVc9F5GqSLelRKq4pKQkOnbsiMViYcyYMURERPDdd9/x4IMPkpaWxuOPPw5AWloaH3zwAYMGDWLkyJGkp6czY8YMevXqxfr162ndunWB/c6aNYusrCweeughfH19CQsLs382YMAA6taty6RJk9i4cSMffPABkZGRvPLKK1et71/+8heqV6/OhAkT+P3335kyZQpjxoxh7ty59jLjxo3j1VdfpW/fvvTq1YvffvuNXr16kZWVVazfyYMPPsjs2bPp06cPI0aMIC8vj59//pm1a9fSrl27Yu3jj+6++24aNmzISy+9hGEY3Hbbbfz9739n3rx5/O1vfytQdt68efTs2ZPq1asD8MMPP9CnTx/atm3LhAkT8PDwYNasWdx00038/PPPdOjQoVR1Eqm0DBGptGbNmmUAxi+//HLFMg8++KARExNjnDp1qsD6e+65xwgJCTEyMzMNwzCMvLw8Izs7u0CZs2fPGlFRUcbw4cPt6w4ePGgARnBwsJGcnFyg/IQJEwygQHnDMIw77rjDqFGjRoF1derUMYYMGXLZuXTv3t2w2Wz29U888YTh6elppKSkGIZhGImJiYaXl5fRv3//Avt7/vnnDaDAPgvzww8/GIDx6KOPXvZZ/nHzz3HWrFmXlQGMCRMmXHbOgwYNuqxsp06djLZt2xZYt379egMwPvroI/sxGzZsaPTq1avAeWdmZhp169Y1evToUeT5iFRFui0lUoUZhsFnn31G3759MQyDU6dO2V+9evUiNTWVjRs3AuDp6YmPjw9g3rY5c+YMeXl5tGvXzl7mUnfddRcRERGFHvfhhx8u8P66667j9OnTpKWlXbXODz30EBaLpcC2VqvVfntn2bJl5OXl8cgjjxTY7i9/+ctV9w3w2WefYbFYmDBhwmWfXXrckvrjOQMMHDiQDRs2sH//fvu6uXPn4uvrS79+/QDYvHkze/fu5d577+X06dP263Pu3DluvvlmfvrpJ2w2W6nrJVIZKdyIVGEnT54kJSWF9957j4iIiAKvYcOGAZCcnGwv/+GHH9KyZUv8/PyoUaMGERERLFy4kNTU1Mv2Xbdu3Sset3bt2gXe599+ubSPSWm3zQ85DRo0KFAuLCzMXrYo+/fvJzY2tsBtNEco7Pdx99134+HhYb+lZhgGn376KX369CE4OBiAvXv3AjBkyJDLrtEHH3xAdnZ2ob9/kapMfW5EqrD8/+O///77GTJkSKFlWrZsCcCcOXMYOnQo/fv3529/+xuRkZF4enoyadKkAi0P+fz9/a94XE9Pz0LXG4Zx1TqXZVtHuVILjtVqveI2hf0+YmNjue6665g3bx7PPPMMa9eu5fDhwwX6HuVfo9dee+2yfk35qlWrVoLai1R+CjciVVhERARBQUFYrVa6d+9eZNn58+dTr149FixYUOCPe2G3b9ypTp06AOzbt69Aa8np06eL1TJUv359Fi9ezJkzZ67YepPfApSSklJgfWFPPl3NwIEDeeSRR9i9ezdz584lICCAvn37FqgPQHBw8FWvkYiYdFtKpArz9PTkrrvu4rPPPmPbtm2XfX7pI9b5LSaXtpCsW7eONWvWOL+iJXDzzTfj5eXFO++8U2D9tGnTirX9XXfdhWEYvPDCC5d9ln/uwcHBhIeH89NPPxX4/O233y5xfe+66y48PT35+OOP+fTTT7ntttsIDAy0f962bVvq16/P66+/TkZGxmXb//ExeBFRy41IlTBz5kwWLVp02frHHnuMl19+meXLl5OQkMDIkSNp1qwZZ86cYePGjSxdupQzZ84AcNttt7FgwQLuuOMObr31Vg4ePMj06dNp1qxZoX903SUqKorHHnuMN954g9tvv53evXvz22+/8d133xEeHn7VTsHdunXjgQceYOrUqezdu5fevXtjs9n4+eef6datG2PGjAFgxIgRvPzyy4wYMYJ27drx008/sWfPnhLXNzIykm7dujF58mTS09MZOHBggc89PDz44IMP6NOnD82bN2fYsGHUrFmTY8eOsXz5coKDg/n6669LfFyRykzhRqQK+GMrRr6hQ4dSq1Yt1q9fz4svvsiCBQt4++23qVGjBs2bNy/Q92Po0KEkJiby7rvvsnjxYpo1a8acOXP49NNPWbFihYvOpHheeeUVAgICeP/991m6dCmdOnXi+++/p2vXrvj5+V11+1mzZtGyZUtmzJjB3/72N0JCQmjXrh2dO3e2lxk/fjwnT55k/vz5zJs3jz59+vDdd98RGRlZ4voOHDiQpUuXEhQUxC233HLZ5zfeeCNr1qxh4sSJTJs2jYyMDKKjo0lISODPf/5ziY8nUtlZDFf2whMRcZOUlBSqV6/OP//5T5599ll3V0dEnEh9bkSk0jl//vxl66ZMmQKYrSAiUrnptpSIVDpz585l9uzZ3HLLLVSrVo2VK1fy8ccf07NnT7p06eLu6omIkynciEil07JlS7y8vHj11VdJS0uzdzL+5z//6e6qiYgLqM+NiIiIVCrqcyMiIiKVisKNiIiIVCpVrs+NzWbj+PHjBAUFlWmGXxEREXEdwzBIT08nNjYWD4+i22aqXLg5fvw4cXFx7q6GiIiIlMKRI0eoVatWkWWqXLgJCgoCzF9OcHCwm2sjIiIixZGWlkZcXJz973hRqly4yb8VFRwcrHAjIiJSwRSnS4k6FIuIiEilonAjIiIilYrCjYiIiFQqVa7PjYiIiLNYrVZyc3PdXY0Ky8fH56qPeReHwo2IiEgZGYZBYmIiKSkp7q5Khebh4UHdunXx8fEp034UbkRERMooP9hERkYSEBCgQWJLIX+Q3RMnTlC7du0y/Q4VbkRERMrAarXag02NGjXcXZ0KLSIiguPHj5OXl4e3t3ep96MOxSIiImWQ38cmICDAzTWp+PJvR1mt1jLtR+FGRETEAXQrquwc9TtUuBEREZFKReFGREREHCI+Pp4pU6a4uxoKNyIiIlWNxWIp8vX888+Xar+//PILDz30kGMrWwp6WspBbDaD0+dyOJedR3x4oLurIyIickUnTpywL8+dO5fx48eze/du+7pq1arZlw3DwGq14uV19cgQERHh2IqWklpuHOTnfado/39LeXjOBndXRUREpEjR0dH2V0hICBaLxf5+165dBAUF8d1339G2bVt8fX1ZuXIl+/fvp1+/fkRFRVGtWjXat2/P0qVLC+z3j7elLBYLH3zwAXfccQcBAQE0bNiQr776yunn5/Zw89ZbbxEfH4+fnx8JCQmsX7++yPJTpkyhcePG+Pv7ExcXxxNPPEFWVpaLantlsSF+ABxPOe/mmoiIiDsZhkFmTp5bXoZhOOw8nn76aV5++WV27txJy5YtycjI4JZbbmHZsmVs2rSJ3r1707dvXw4fPlzkfl544QUGDBjAli1buOWWW7jvvvs4c+aMw+pZGLfelpo7dy5jx45l+vTpJCQkMGXKFHr16sXu3buJjIy8rPz//vc/nn76aWbOnEnnzp3Zs2cPQ4cOxWKxMHnyZDecwUXRF8JNWlYe57LzCPTVHT8RkarofK6VZuMXu+XYO17sRYCPY/7+vPjii/To0cP+PiwsjFatWtnfT5w4kc8//5yvvvqKMWPGXHE/Q4cOZdCgQQC89NJLTJ06lfXr19O7d2+H1LMwbm25mTx5MiNHjmTYsGE0a9aM6dOnExAQwMyZMwstv3r1arp06cK9995LfHw8PXv2ZNCgQVdt7XGFID9vgi4EmhOp7m9JEhERKYt27doVeJ+RkcGTTz5J06ZNCQ0NpVq1auzcufOqLTctW7a0LwcGBhIcHExycrJT6pzPbc0LOTk5bNiwgXHjxtnXeXh40L17d9asWVPoNp07d2bOnDmsX7+eDh06cODAAb799lseeOCBKx4nOzub7Oxs+/u0tDTHncQfRIf4kZ6cQWJqFg0iq119AxERqXT8vT3Z8WIvtx3bUQIDCz4c8+STT7JkyRJef/11GjRogL+/P3/605/Iyckpcj9/nEbBYrFgs9kcVs/CuC3cnDp1CqvVSlRUVIH1UVFR7Nq1q9Bt7r33Xk6dOkXXrl0xDIO8vDwefvhhnnnmmSseZ9KkSbzwwgsOrfuVxIT6szc5g+Op6ncjIlJVWSwWh90aKk9WrVrF0KFDueOOOwCzJef33393b6WuwO0diktixYoVvPTSS7z99tts3LiRBQsWsHDhQiZOnHjFbcaNG0dqaqr9deTIEafVLybY7HeTqNtSIiJSyTRs2JAFCxawefNmfvvtN+69916nt8CUltuiZXh4OJ6eniQlJRVYn5SURHR0dKHbPPfcczzwwAOMGDECgBYtWnDu3Dkeeughnn32WTw8Ls9qvr6++Pr6Ov4EChETaoabE2q5ERGRSmby5MkMHz6czp07Ex4ezlNPPeXUrh5l4bZw4+PjQ9u2bVm2bBn9+/cHwGazsWzZsiv2us7MzLwswHh6mvcXHfn4W2nFhOSHG7XciIhIxTB06FCGDh1qf3/jjTcW+jc1Pj6eH374ocC60aNHF3j/x9tUhe0nJSWl1HUtLrfeFBw7dixDhgyhXbt2dOjQgSlTpnDu3DmGDRsGwODBg6lZsyaTJk0CoG/fvkyePJk2bdqQkJDAvn37eO655+jbt6895LhTTIg/ACdSFG5ERETcxa3hZuDAgZw8eZLx48eTmJhI69atWbRokb2T8eHDhwu01PzjH//AYrHwj3/8g2PHjhEREUHfvn35v//7P3edQgEXW250W0pERMRdLEZ5uJ/jQmlpaYSEhJCamkpwcLBD952Rncc1E8yBm7a/0EsD+YmIVAFZWVkcPHiQunXr4ufn5+7qVGhF/S5L8ve7Qj0tVd5V8/XSQH4iIiJupnDjYHpiSkRExL0UbhwsOr9TsVpuRERE3ELhxsHyZwfXE1MiIiLuoXDjYPmzgyem6baUiIiIOyjcOFjshdtSx9VyIyIi4hYKNw5mb7lRnxsRERG3ULhxsNgLT0tpZnARESmvLBZLka/nn3++TPv+4osvHFbX0tAocw6W/7RUelYeGdl5VNNAfiIiUs6cOHHCvjx37lzGjx/P7t277euqVavmjmo5jFpuHKyarxdBfmagSVTrjYiIlEPR0dH2V0hICBaLpcC6Tz75hKZNm+Ln50eTJk14++237dvm5OQwZswYYmJi8PPzo06dOvY5IOPj4wG44447sFgs9veupmYFJ4gJ8SM9K4MTqVk0iAxyd3VERMSVDANyM91zbO8AsFjKtIv//ve/jB8/nmnTptGmTRs2bdrEyJEjCQwMZMiQIUydOpWvvvqKefPmUbt2bY4cOcKRI0cA+OWXX4iMjGTWrFn07t3bbZNaK9w4QUyIP3uSMjTWjYhIVZSbCS/FuufYzxwHn8Ay7WLChAm88cYb3HnnnQDUrVuXHTt28O677zJkyBAOHz5Mw4YN6dq1KxaLhTp16ti3jYiIACA0NJTo6Ogy1aMsFG6c4OLs4Ao3IiJScZw7d479+/fz4IMPMnLkSPv6vLw8QkJCABg6dCg9evSgcePG9O7dm9tuu42ePXu6q8qFUrhxghj7FAzqcyMiUuV4B5gtKO46dhlkZGQA8P7775OQkFDgs/xbTNdeey0HDx7ku+++Y+nSpQwYMIDu3bszf/78Mh3bkRRunEAtNyIiVZjFUuZbQ+4SFRVFbGwsBw4c4L777rtiueDgYAYOHMjAgQP505/+RO/evTlz5gxhYWF4e3tjtVpdWOvLKdw4gWYGFxGRiuqFF17g0UcfJSQkhN69e5Odnc2vv/7K2bNnGTt2LJMnTyYmJoY2bdrg4eHBp59+SnR0NKGhoYD5xNSyZcvo0qULvr6+VK9e3eXnoEfBnUAtNyIiUlGNGDGCDz74gFmzZtGiRQtuuOEGZs+eTd26dQEICgri1VdfpV27drRv357ff/+db7/9Fg8PM1K88cYbLFmyhLi4ONq0aeOWc7AYhmG45chukpaWRkhICKmpqQQHBzvlGBnZeVwzYTEA217opYH8REQqsaysLA4ePEjdunXx8/Nzd3UqtKJ+lyX5+62WGyfQQH4iIiLuo3DjJJodXERExD0UbpxEs4OLiIi4h8KNk2h2cBEREfdQuHGS6GDztpRabkREqoYq9nyOUzjqd6hw4yQx9pYbhRsRkcrM29sbgMxMN02WWYnk5OQAlHnCTT2j7CQx9j43ui0lIlKZeXp6EhoaSnJyMgABAQFYyjgzd1Vks9k4efIkAQEBeHmVLZ4o3DiJfX4pPS0lIlLp5c+AnR9wpHQ8PDyoXbt2mcOhwo2T5LfcpGfnkZ6VS5Cft5trJCIizmKxWIiJiSEyMpLc3Fx3V6fC8vHxsY90XBYKN04S6OtFsJ8XaVl5JKZmKdyIiFQBnp6eZe4vImWnDsVOZL81pU7FIiIiLqNw40SaHVxERMT1FG6cSLODi4iIuJ7CjRPpiSkRERHXU7hxovz5pU6kKdyIiIi4isKNE8XaW27U50ZERMRVFG6cSDODi4iIuJ7CjRP9cSA/ERERcT6FGyfKH8gP1HojIiLiKgo3ThYbqoH8REREXEnhxsnsT0xpID8RERGXULhxMg3kJyIi4loKN06mgfxERERcS+HGyTSQn4iIiGsp3DiZBvITERFxLYUbJ9NAfiIiIq6lcONkGshPRETEtRRunEwD+YmIiLiWwo0L5A/kd1zhRkRExOkUblzgYr8bdSoWERFxNoUbF8gf6+a4xroRERFxOoUbF4jRE1MiIiIuo3DjAvnh5rhuS4mIiDidwo0L5N+WUsuNiIiI8yncuEBMqCbPFBERcRWFGxfIvy2VoYH8REREnE7hxgUCfLwI8fcG1HojIiLibAo3LpLfeqNwIyIi4lwKNy5iDzeaHVxERMSpFG5cJPrCE1NquREREXEuhRsXibXfllLLjYiIiDMp3LhItPrciIiIuITCjYvkzwyucCMiIuJcCjcuEq35pURERFxC4cZFLh3IL00D+YmIiDhNuQg3b731FvHx8fj5+ZGQkMD69euvWPbGG2/EYrFc9rr11ltdWOOSu3QgP7XeiIiIOI/bw83cuXMZO3YsEyZMYOPGjbRq1YpevXqRnJxcaPkFCxZw4sQJ+2vbtm14enpy9913u7jmJWefHVxj3YiIiDiN28PN5MmTGTlyJMOGDaNZs2ZMnz6dgIAAZs6cWWj5sLAwoqOj7a8lS5YQEBBQocKNWm5EREScx63hJicnhw0bNtC9e3f7Og8PD7p3786aNWuKtY8ZM2Zwzz33EBgY6KxqOkzMhSemjivciIiIOI2XOw9+6tQprFYrUVFRBdZHRUWxa9euq26/fv16tm3bxowZM65YJjs7m+zsbPv7tLS00le4jGKC81tudFtKRETEWdx+W6osZsyYQYsWLejQocMVy0yaNImQkBD7Ky4uzoU1LChGY92IiIg4nVvDTXh4OJ6eniQlJRVYn5SURHR0dJHbnjt3jk8++YQHH3ywyHLjxo0jNTXV/jpy5EiZ611amhlcRETE+dwabnx8fGjbti3Lli2zr7PZbCxbtoxOnToVue2nn35KdnY2999/f5HlfH19CQ4OLvByl0tnBjcMw231EBERqczcfltq7NixvP/++3z44Yfs3LmTUaNGce7cOYYNGwbA4MGDGTdu3GXbzZgxg/79+1OjRg1XV7nUYi7MDH4ux0p6dp6bayMiIlI5ubVDMcDAgQM5efIk48ePJzExkdatW7No0SJ7J+PDhw/j4VEwg+3evZuVK1fy/fffu6PKpebv40logDcpmbmcSMkiONrb3VUSERGpdCxGFbs/kpaWRkhICKmpqW65RdV7yk/sSkxn9rD23Ng40uXHFxERqYhK8vfb7belqhrNDi4iIuJcCjcuFq0npkRERJxK4cbFYi95YkpEREQcT+HGxaIvPDGVmKaWGxEREWdQuHGxWM0MLiIi4lQKNy52aZ+bKvagmoiIiEso3LhY/kB+mTlW0rI0kJ+IiIijKdy4WP5AfgCJemJKRETE4RRu3CC/9eZ4qvrdiIiIOJrCjRvkT6CplhsRERHHU7hxgxiNdSMiIuI0CjduEKNRikVERJxG4cYN8vvcKNyIiIg4nsKNG1xsudFtKREREUdTuHGDmEtmBtdAfiIiIo6lcOMG0cFmy40G8hMREXE8hRs38PfxpLoG8hMREXEKhRs3idZAfiIiIk6hcOMmsRrIT0RExCkUbtwkWgP5iYiIOIXCjZvEhmqsGxEREWdQuHGT/CemFG5EREQcS+HGTWJCNZCfiIiIMyjcuMmlUzBoID8RERHHUbhxk/wpGDSQn4iIiGMp3LiJn/fFgfx0a0pERMRxFG7cSLODi4iIOJ7CjRvZZwdPUbgRERFxFIUbN4q2j1Ks21IiIiKOonDjRvkD+R3XbSkRERGHUbhxo/yB/DS/lIiIiOMo3LhR/kB+mhlcRETEcRRu3Cj/aalEDeQnIiLiMAo3blRgIL/zGshPRETEERRu3KjAQH5pujUlIiLiCAo3bmYfyE9j3YiIiDiEwo2b2Qfy0xNTIiIiDqFw42b5T0xpfikRERHHULhxM80vJSIi4lgKN2528baUWm5EREQcQeHGzaLV50ZERMShFG7cLPaSp6U0kJ+IiEjZKdy4WX7LzflcDeQnIiLiCAo3bubn7UlYoA+gOaZEREQcQeGmHNDs4CIiIo6jcFMOxGp2cBEREYdRuCkH8vvdqOVGRESk7BRuyoH8gfyOa34pERGRMlO4KQfyB/JL1MzgIiIiZaZwUw5oZnARERHHUbgpBy6dGVwD+YmIiJSNwk05cOlAfqnnc91cGxERkYpN4aYcuHQgP80xJSIiUjYKN+WEZgcXERFxDIWbciJGs4OLiIg4hMJNOaEnpkRERBxD4aaciFbLjYiIiEMo3JQT+fNLqc+NiIhI2SjclBPRweZtKc0vJSIiUjYKN+XEpTODayA/ERGR0lO4KSeigs1wk5Vr00B+IiIiZaBwU074eXtS48JAfpodXEREpPQUbsqRaM0OLiIiUmYKN+VI/lg3arkREREpPbeHm7feeov4+Hj8/PxISEhg/fr1RZZPSUlh9OjRxMTE4OvrS6NGjfj2229dVFvnyh+lWE9MiYiIlJ6XOw8+d+5cxo4dy/Tp00lISGDKlCn06tWL3bt3ExkZeVn5nJwcevToQWRkJPPnz6dmzZocOnSI0NBQ11feCWIueWJKRERESset4Wby5MmMHDmSYcOGATB9+nQWLlzIzJkzefrppy8rP3PmTM6cOcPq1avx9vYGID4+3pVVdiq13IiIiJSd225L5eTksGHDBrp3736xMh4edO/enTVr1hS6zVdffUWnTp0YPXo0UVFRXHPNNbz00ktYrdYrHic7O5u0tLQCr/LKPr+Uwo2IiEipuS3cnDp1CqvVSlRUVIH1UVFRJCYmFrrNgQMHmD9/PlarlW+//ZbnnnuON954g3/+859XPM6kSZMICQmxv+Li4hx6Ho50cWZwDeQnIiJSWm7vUFwSNpuNyMhI3nvvPdq2bcvAgQN59tlnmT59+hW3GTduHKmpqfbXkSNHXFjjktFAfiIiImXntj434eHheHp6kpSUVGB9UlIS0dHRhW4TExODt7c3np6e9nVNmzYlMTGRnJwcfHx8LtvG19cXX19fx1beSfIH8jt9LofjKVmEBlx+PiIiIlK0ErXcrF+//qr9W+bNm1esffn4+NC2bVuWLVtmX2ez2Vi2bBmdOnUqdJsuXbqwb98+bDabfd2ePXuIiYkpNNhURPlPTGkgPxERkdIpUbjp1KkTp0+ftr8PDg7mwIED9vcpKSkMGjSo2PsbO3Ys77//Ph9++CE7d+5k1KhRnDt3zv701ODBgxk3bpy9/KhRozhz5gyPPfYYe/bsYeHChbz00kuMHj26JKdRruXPDq6B/EREREqnRLel/tjJtbBOryXpCDtw4EBOnjzJ+PHjSUxMpHXr1ixatMjeyfjw4cN4eFzMX3FxcSxevJgnnniCli1bUrNmTR577DGeeuqpkpxGuZY/O7geBxcRESkdh/e5sVgsJSo/ZswYxowZU+hnK1asuGxdp06dWLt2bWmqViHkzy+lgfxERERKp0I9LVUVxF4Y60YtNyIiIqVT4pabHTt22MehMQyDXbt2kZGRAZhj10jZRNvHulG4ERERKY0Sh5ubb765QL+a2267DTBvRxmGUeLbUlJQrH2U4vP6fYqIiJRCicLNwYMHnVUPuSAqxByTJyvXRkpmLtUDK8cj7iIiIq5SonBTp06dq5bZtm1bqSsj4OvlSXg1H05l5HAiNUvhRkREpIQc0qE4PT2d9957jw4dOtCqVStH7LJKi75kjikREREpmTKFm59++okhQ4YQExPD66+/zk033VSpH9N2Fc0OLiIiUnol7lCcmJjI7NmzmTFjBmlpaQwYMIDs7Gy++OILmjVr5ow6VjkxarkREREptRK13PTt25fGjRuzZcsWpkyZwvHjx3nzzTedVbcqSy03IiIipVeilpvvvvuORx99lFGjRtGwYUNn1anKs7fcaH4pERGREitRy83KlStJT0+nbdu2JCQkMG3aNA3c5wT54SYxTeFGRESkpEoUbjp27Mj777/PiRMn+POf/8wnn3xCbGwsNpuNJUuWkJ6e7qx6Vin5t6WOp5wv0USkIiIiUsqnpQIDAxk+fDgrV65k69at/PWvf+Xll18mMjKS22+/3dF1rHLyB/LLzjMH8hMREZHiK/M4N40bN+bVV1/l6NGjfPLJJ5ouwAHyB/IDzQ4uIiJSUiXqUDx8+PCrlqlRo0apKyMXxYT4cyojh8TULJrHhri7OiIiIhVGicLN7NmzqVOnDm3atLliXxC13DhGdIgfW4+lclyPg4uIiJRIicLNqFGj+Pjjjzl48CDDhg3j/vvvJywszFl1q9LsT0zptpSIiEiJlKjPzVtvvcWJEyf4+9//ztdff01cXBwDBgxg8eLFeqrHwewD+WmsGxERkRIpcYdiX19fBg0axJIlS9ixYwfNmzfnkUceIT4+noyMDGfUsUq6OAWDwo2IiEhJlOlpKQ8PDywWC4ZhYLVaHVUnQfNLiYiIlFaJw012djYff/wxPXr0oFGjRmzdupVp06Zx+PBhqlWr5ow6VkmXzi+lW34iIiLFV6IOxY888giffPIJcXFxDB8+nI8//pjw8HBn1a1Ku3Qgv7OZuYQF+ri5RiIiIhVDicLN9OnTqV27NvXq1ePHH3/kxx9/LLTcggULHFK5qix/IL9TGTmcSD2vcCMiIlJMJQo3gwcP1jg2LpQ/kN+JFA3kJyIiUlwlHsRPXCd/IL8Tmh1cRESk2Mo8t5RcwpoLGScdtrvY/CemUvTElIiISHEp3DjKgRUwtQ18/ZjDdhl94YmpRI11IyIiUmwlui0lRQiKhdSjkHoEkndBZJMy7zI21Gy50czgIiIixaeWG0eJaARNbzOXV/3bIbuMDs6fX0otNyIiIsWlcONIXZ4wf26dBylHyry72FAN5CciIlJSCjeOVKst1L0ebHmwZlqZdxcZXHAgPxEREbk6hRtH6zrW/LnhQzh3uky7MgfyMwPOcT0xJSIiUiwKN45W70aIaQ1552H9u2XeXf4Emup3IyIiUjwKN45msUDXC31v1r0L2Rll2p1mBxcRESkZhRtnaNoXwupDVgps/LBMu7oYbtRyIyIiUhwKN87g4QldLgzmt3oa5OWUelcxlzwxJSIiIlencOMsre6BatGQftx8NLyUdFtKRESkZBRunMXLFzqNNpdXTgGbtVS7iQlRy42IiEhJKNw4U7th4BcCp/fCroWl2sWlfW40kJ+IiMjVKdw4k28QdHjIXF75LyhFOIm6MAVDTp6NM+dK33dHRESkqlC4cbaEh8HLH45vhIM/lXhzHy8P+0B+ujUlIiJydQo3zhYYDtc+YC6v/FepdpE/O7jCjYiIyNUp3LhCpzFg8YQDy+H4phJvfnF2cD0xJSIicjUKN65QvQ60+JO5vHJKiTfPnx38uFpuRERErkrhxlW6PG7+3PElnNpXok2jNb+UiIhIsSncuEpUM2jUBzBg9b9LtGn+4+CaGVxEROTqFG5cKX9Czc0fQ9rxYm+WP5BfYppabkRERK5G4caVaidA7c5gy4W1bxd7Mw3kJyIiUnwKN66W33rz6yw4f7ZYm0QF+2GxaCA/ERGR4lC4cbWGPSDqGsjJgF8+KNYmGshPRESk+BRuXM1iudh6s3Y65GQWa7NLb02JiIjIlSncuEOz/hBaBzJPwaY5xdokJkQD+YmIiBSHwo07eHpBl0fN5dVvgjX3qpvkPzG1/+Q5Z9ZMRESkwlO4cZfW90FgBKQehm0Lrlq8VVwIAP9Ze4jvtyc6u3YiIiIVlsKNu3j7Q8dR5vLKf4HNVmTx/q1rcnfbWlhtBmM+3sSa/addUEkREZGKR+HGndo9CD5BcHIn7P2+yKIWi4VJd7agZ7MocvJsjPzoV7YcTXFNPUVERCoQhRt38g+F9sPN5ZX/umpxL08Ppg5qQ+f6NcjIzmPIzPXsS053bh1FREQqGIUbd+v4CHj6wpG1cGjNVYv7eXvy3uB2tKoVwtnMXB6YsZ6jZ4v3OLmIiEhVoHDjbkHR0Ppec3nl5GJtUs3Xi9nDOtAgshonUrN4YMZ6TmVkO7GSIiIiFYfCTXnQ+S9g8TD73SRuK9Ym1QN9+M+DHagZ6s/BU+cYMnM9aVlXf6RcRESkslO4KQ9q1DcH9gNYNaXYm8WE+DNnRALh1XzYfjyNEbN/JSvX6pQqioiIVBQKN+VF18fNn9s+gzMHi71Z3fBAPhzegSBfL9b/foZH/ruRXGvRj5WLiIhUZgo35UVMK6h/Mxg2WDOtRJs2jw1h5rD2+Hl78MOuZP726W/YbIaTKioiIlK+lYtw89ZbbxEfH4+fnx8JCQmsX7/+imVnz56NxWIp8PLz83NhbZ3ourHmz01zICO5RJu2jw/jnfva4uVh4YvNx3nh6+0YhgKOiIhUPW4PN3PnzmXs2LFMmDCBjRs30qpVK3r16kVy8pX/uAcHB3PixAn769ChQy6ssRPV6QK12kNeFqybXuLNuzWJ5I0BrbBY4MM1h/jX0r1OqKSIiEj55vZwM3nyZEaOHMmwYcNo1qwZ06dPJyAggJkzZ15xG4vFQnR0tP0VFRXlwho7kcUCXZ8wl9d/AFlpJd5Fv9Y1ebHfNQBMXbaXmSuL339HRESkMnBruMnJyWHDhg10797dvs7Dw4Pu3buzZs2VB7TLyMigTp06xMXF0a9fP7Zv3+6K6rpGoz4Q3hiyU+HXKwe8ojzQsQ5P9mwEwIvf7OCzDUcdWUMREZFyza3h5tSpU1it1staXqKiokhMLHzm68aNGzNz5ky+/PJL5syZg81mo3Pnzhw9Wvgf8OzsbNLS0gq8yjUPj4tPTq19G3KzSrWb0d0a8GDXugD8/bMtmklcRESqDLffliqpTp06MXjwYFq3bs0NN9zAggULiIiI4N133y20/KRJkwgJCbG/4uLiXFzjUrjmTxBcCzKS4LePS7ULi8XCs7c05U+aSVxERKoYt4ab8PBwPD09SUpKKrA+KSmJ6OjoYu3D29ubNm3asG/fvkI/HzduHKmpqfbXkSNHylxvp/Pygc5jzOVV/wZb6Qbm8/Cw8LJmEhcRkSrGreHGx8eHtm3bsmzZMvs6m83GsmXL6NSpU7H2YbVa2bp1KzExMYV+7uvrS3BwcIFXhXDtYPAPg7MHYceXpd6NZhIXEZGqxu23pcaOHcv777/Phx9+yM6dOxk1ahTnzp1j2LBhAAwePJhx48bZy7/44ot8//33HDhwgI0bN3L//fdz6NAhRowY4a5TcA6fQEh42Fxe+S8ow5g1mklcRESqEreHm4EDB/L6668zfvx4WrduzebNm1m0aJG9k/Hhw4c5ceKEvfzZs2cZOXIkTZs25ZZbbiEtLY3Vq1fTrFkzd52C83QYCd6BkLgF9v9Qpl1V8/VilmYSFxGRKsBiVLFhbNPS0ggJCSE1NbVi3KJa9AysfQvir4Oh35R5dydSz/Ond9ZwLOU8zWOD+fihjgT7eTugoiIiIs5Tkr/fbm+5kavoNBo8vOH3n+HIL2XeXUyIP/95sINmEhcRkUpL4aa8C6kJLQeay6umOGSX9SKqMXuYZhIXEZHKSeGmIujyGGCBXd/Ayd0O2eU1NUOYMbQ9vl6aSVxERCoXhZuKIKIRNL3NXP78YchKdchuO9QN4537r7XPJP68ZhIXEZFKQOGmorj5eXPcm+Mb4b93Q7Zjxqq5qUmUfSbxj9YcYtD7a9l/MsMh+xYREXEHhZuKIrwBDP4S/ELhyDr430DIOeeQXfdrXZNX7mqJv7cnaw+coc+Un/n30r1k56mjsYiIVDwKNxVJTEt44HPwDYZDq+DjeyD3vEN2PaBdHN8/cT03No4gx2rjX0v3cMu/f2bdAc1HJSIiFYvCTUVT81q4fwH4VIODP8En95Z65vA/igsLYNbQ9rw5qA3h1XzZf/IcA99by1Pzt5CSmeOQY4iIiDibwk1FFNce7psP3gHmyMXzBkOeY0Ybtlgs9G0Vy7KxNzCoQ20A5v56hO6Tf+TLzcfU4VhERMo9hZuKqk4nuHceePnD3sXw6TCw5jps9yEB3ky6swWfPtyJhpHVOJWRw2OfbGbwzPUcPq15qUREpPxSuKnI6l4Hgz4GT1/YvRA+exCseQ49RPv4MBY+eh1P9myEj5cHP+89Rc8pP/LOiv0a+E9ERMolhZuKrn43uOd/4OkDO76Ez/8MNsc+5eTj5cGYmxqy+PHr6Vy/Blm5Nl5ZtIu+b65k4+GzDj2WiIhIWSncVAYNu8OAj8DDC7bNhy9Hg83xrSp1wwP574gE3ri7FdUDvNmVmM5d76zmuS+2kZbluFtiIiIiZaFwU1k07gN/mgUWT/jtY/jmMacEHIvFwl1ta7Hsrzdy17W1MAz4z9pD9Jj8I99tPaEOxyIi4nYKN5VJs9vhrvfB4gEbP4JvnwQnhY2wQB/eGNCK/41IoG54IElp2Yz670ZGfvQrx1IcM/aOiIhIaSjcVDbX3AX9pwMW+HUGLBrntIAD0LlBON89dh2P3tQAb08LS3cm02Pyj8xYeRCrJuIUERE3ULipjFoNhH7TzOV178CS55wacPy8PRnbszHfPnod7eOrk5ljZeI3O+j/1iq2HXPMJJ8iIiLFpXBTWbW5H26bYi6vfhN+mOjUgAPQMCqIuQ91YtKdLQj282LrsVRun7aSid/s4Fy2Yx9RFxERuRKFm8qs3TC45XVz+ec34MdXnH5IDw8LgzrUZulfb6Bvq1hsBsxYeZCe//qJH3YlOf34IiIiCjeVXYeR0Oslc3nFJPjpdZccNjLIjzcHtWH2sPbUqu7PsZTzDJ/9K09++pseGxcREadSuKkKOo2G7i+Yyz9MhFVTXXboGxtHsuSJGxh5XV0sFpi/4Sh9pvzM6n2nXFYHERGpWhRuqoquj0O3f5jLS56DtdNddmh/H0+evbUZ8/7cidphARxLOc+9H6zj+a+2cz7HsaMpi4iIKNxUJTf8Da7/u7m86Cn45QOXHr59fBjfPXYd93c0Zxufvfp3bpn6MxsOaQoHERFxHIWbqqbbM9DlcXN54V/Nwf5cKNDXi3/2b8FHwzsQHezHwVPnuHv6al5ZtIvsPLXiiIhI2SncVDUWC3R/HjqONt9/9Shs/tjl1bi+UQSLn7ieO9vUxGbAOyv202/aKnYcT3N5XUREpHJRuKmKLBbo9X/QfiRgwJePwNb5Lq9GiL83kwe2Zvr9bakR6MOuxHT6vbWSaT/sJc/q+HmxRESkalC4qaosFujzKrQdCoYNFjwE2z93S1V6XxPN4ieup1fzKHKtBq9/v4e7pq9h/8kMt9RHREQqNotRxaZxTktLIyQkhNTUVIKDg91dHfez2eCrv8DmOeb7iCYQ1wHiOkJcAtSobwYhFzAMg883HWPCV9tJz8rD18uDp3o3YWjneDw8XFMHEREpn0ry91vhRsBmhW+egI0fXv6Zf5gZcuI6mD9j24BPgFOrcyL1PH+fv4Wf95pj4XSsF8Zrf2pFXJhzjysiIuWXwk0RFG6KcO4UHFkPR9aaP49tBGt2wTIeXhDd0gw6tRPMn8GxDq+KYRjMWXeYlxbu5HyulWq+Xjx3W1MGtIvD4qKWJBERKT8UboqgcFMCeTmQuAWOrDNfh9dBRuLl5ULiLrbsxHWAqBbg6eWQKhw6fY6/zvuNXy+MhXNTk0hevrMFkcF+Dtm/iIhUDAo3RVC4KQPDgJTDF1p3LgSepG1mh+RLeQdAzbYXwk4C1GoHAWGlPqzVZvDBzwd44/s95FhthAZ4M7HfNfRt5fgWIylncs/D4TVQ90bw0PMPIlWZwk0RFG4cLDsDjm24GHaO/ALZqZeXC28MTW4xR0guZZ+dPUnpjJ23mW3HzLFwbmsZw8R+11A90KcsZyDllWHAf/8E+5ZCj4nQ5VF310hE3EjhpggKN05ms8Gp3ReCzoUWntP7Ln4e0QTumgHR15Rq97lWG9N+2Me05fuw2gwignx55a4W3NQkykEnIOXGhg/h6wuBJigGHtsCXgqyIlWVwk0RFG7c4NwpOLACFj8DGUng6WPOUp7wcKlvNWw5msLYeb+xL9kcC2dAu1o82bOx+uJUFilH4O1OkJMOFk8wrNB/OrQe5O6aiYibKNwUQeHGjc6dgi/HwJ7vzPf1b4b+70BQ6VpdsnKtvL54NzNWHST/X3FMiB8taobQslYILWqF0qJmCGG6bVWxGAb8p78ZiOMSoGEP+OGfENkcRq1y2bhLIlK+KNwUQeHGzQwDfp0Bi5+FvCwICIf+b0OjXqXe5boDp5m4cAfbj6dR2L/mWtX9zbBTM5SWtUK4JjaEkADvMpyEONWvM81xl7z84eGVZmf0fzWH3Ex44HOof5O7aygibqBwUwSFm3IieRd89qD5tBWY81z1nAje/qXeZUZ2HjuOp7HlaApbj6Wy9WgqB06dK7RsfI0AWtQKpWXNEFrUCqF5bDBBfgo8bnf2d3i7M+Seg94vQ8dR5vpv/wbr3zNb+x5Y4NYqioh7KNwUQeGmHMnNgmUvwNq3zfcRTeFPMyCqucMOkZaVy7YLQWfLhZ+Hz2ReVs5igbrhgRfCjtnC0zw2mAAfx4zXI8Vgs8FHt8PvP0PtzjB04cU+WWcOwNRrAQNGrYGoZm6tqoi4nsJNERRuyqG9S+GLUXAuGTx9oceLkPBnp/WtSMnMYeuxVLYcNcPO1mOpHEs5f1k5Dws0iKxGi5qhNI0JwsfLA6vNwGaAzWZgNQzz/YXli+vAduEzs7xx4T2FlDU/Cwv0YWjneBpEBjnlnCuE9e/Dt0+a4ySNWgVh9Qp+PvcB2PkVtL4f+r/lnjqKiNso3BRB4aacyjgJX46GvYvN9w16mH1xqkW65PCnMrLZeiyVbZe08CSmZbnk2Pk8LNCvdU0eu7kh8eGBLj222505AO90MfvV3PI6dBh5eZkj62FGD/Npu8e3lbojuohUTAo3RVC4KccMA375wOxsbM2GwAjo9zY06umW6iSnZdlbePYlZ2Bg4GGx4OlhwdNiwcPDgocFPD0s9vX2z+3L4GmxYClk/aXbrNx3iiU7kgBzf3ddW5O/3NSwakwWarPB7Fvh8GqIvw4Gf3XlIQI+6AFH18N1T8LNz7m2niLiVgo3RVC4qQCSdpidjZN3mO8THjbHxfGu3GPYbD2ayuQlu1m++yQAXh4WBraPY8xNDYgJKX1H63Jv7Tuw6GnwDoRHVkP1+CuX3fElzBsM/tXhie3gU8VauESqsJL8/dZkLVL+RDWDkcvNUAOwbjq8f5MZeiqxFrVCmDWsA5+N6kzXBuHk2Qz+u+4wN7y6gue/2k6yi2+TucSpfbD0BXO558Sigw1Ak9vMMufPwub/Obt2IlJBKdxI+eTtB31egXs/NW9PJW+H97vBuvcodDCbSqRtnerMGZHA3Ic60qFuGDlWG7NX/871ry3n/xbu4HRGtrur6Bg2K3z5COSdh3o3QrvhV9/GwxM6PmIur33b3IeIyB/otpSUfxnJ8MUjsG+J+b5Rb7h9GlSLcG+9XMAwDFbvP80b3+9m4+EUAAJ8PBnaOZ6Hrq9HaEAFHn159TT4/lnwCTJvR4XWLt522Rnwr2aQlQoD/wtNb3NuPUWkXNBtKalcqkXCfZ9C71fMR8X3LIJ3OpuzRVdyFouFLg3C+WxUZ2YNa0/LWiFk5lh5e8V+ur6ynH8t2UNaVq67q1lyJ/fADxPN5V7/V/xgA+Bb7WIrz5ppjq+biFR4armRiiVpO8x/EE7uNN93fARunlDpOxvnMwyDJTuSmLxkD7sS0wEI9vPioevrMbRLXar5VoBBB21WmNETjv1qjjh8/2clH9Mo7QRMaQG2XBjxA9Rq65y6iki5oZYbqbyimsNDy6HDQ+b7tW/DBzeb0zlUARaLhZ7No/n20et4+75raRhZjbSsPF7/fg/Xv7qcd3/cz/mcct4PZfWbZrDxDYHb3yzdYI3BMdDiT+bymjcdWz8RqfDUciMV1+5FZofUzNPg5Wfeqqh3I9TuBH5V49pabQbfbDnOlKV7OXhhHq3war48cmN97k2ojZ+3p5tr+AfJu+Dd68CaY45h1Oa+0u8rcStM7woWD3h0M1Sv47Bqikj5o3FuiqBwU8mkJ5lTN+xfdnGdxRNqXgt1rzdfcQllmpCzIsiz2vh80zGm/rCXI2fMqSSig/0YfVMDBraLw8erHDTSWvNgRnc4vgka9oJ755Z9io2P+sGBFebtyd6THFJNESmfFG6KoHBTCdlssOsbs4PxwZ/g7MGCn3v6mAEnP+zEXgteFfgpoyLkWm18+utRpv2wl+Op5rg4NUP9Gd61Lt2bRlKnhhsHvfvpdbMTsV8IPLLOvLVUVnuXwn/vAp9q5qB+/qFl36eIlEsKN0VQuKkCUg7DwZ/NoHPwJ0g/XvBz7wDz1lV+2IlpZY6fUolk51mZ+8sRpv2wj+T0i+Pi1AsP5IbGEXRrHEmHumGuu22VtB3evcHsAHzHu9DqHsfs1zDg7U5mB/MeL0KXxxyzXxEpdxRuiqBwU8UYBpzeDwd/hN8vBJ7M0wXL+IZAfNeLYSeyqdNmJHe1rFwr8349wndbE/nl9zPk2S5+3f29PenSoAY3NI6kW+MIalV30jxW1lxzhOnELdD4Frjnf479/W78D3w1BoJrwmO/gae34/YtIuWGwk0RFG6qOJvN/L/8/Fad31dCdlrBMoER5gSO+WEnrF6lCDvpWbms2neaFbuTWb47maS0giMdN4ysRrcmkdzYOIJ2dcIc109nxSuw4iVzPqhH1jl+Nu+8bPjXNXAuGe58H1oOcOz+RaRcULgpgsKNFGDNg8TfLoadw2shN7NgmeCaUKez2VfEw+vCy/OSZS/ziZ1L3//x80LXXfLev7pLW4wMw2DniXRW7Elmxa6TbDh8FuslrTqBPp50bRhOt8aR3Ng4kuiQUo4jdGKLOW2GLQ/umnHx8W1H+/E1WP5PiG4Jf/6pUoRRESlI4aYICjdSpLwcOLbhYtg5ut58bNkVgmKgYU9zeol6N7h0xuvUzFx+3neS5btO8uOek5z6w/xVTaKD6NYkkm6NI7m2dihensVo1cnLuTDh6VZo2hcG/Md5oSPzDExuZs5TNeRrs8VNRCoVhZsiKNxIieRkwpF1cHyj+cfalnfJy1rwvWEr+vPL3l+yLvVowRYjT1+oe50ZdBr2dOkYLjabwfbjaSy/cPtq85GUAnOVBvl5cX3DCG5sHMENjSOIDLpCq87yl+DHVyCghnk7qhhzgRmGgc2APJsNq80gz2bg7eGBv08xOj4v/Cv88oH5mPl984p5tk52ZD34BkNkE3fXRKTCU7gpgsKNlEu5WXBoJexZbM6dlXK44OcRTaFRLzPs1GoPnq6bZuHMuRx+3nuS5buS+XHPSc5mFpzLqnlsMOHVfC+EETOU1M7ey6tnH8cTG/8MeIrlnp3tYaXAT6vtsvV/5Olh4f6E2jzZqzFBfkV0Fj69H95sCxgwej1ENHbwb6KENs2BL0ebyy0Hwk3PQWice+skUoEp3BRB4UbKPcOAk7vNkLP3e7MfkHHJlAp+odCwhxl06t8EAWEuq5rVZvDb0RRW7EpmxZ6TbDmaelkZH3L5yucfNPE4wjfWjozJfdQhx44O9uPFfs3p2Tz6yoU+uc8c8+jaIXD7VIcct1T2LYX/Dih43Tx9oePD0HWsxuMRKQWFmyIo3EiFc/4s7FtmtursW2K+z2fxgLiOF1p1ekFEE5d2pj2Zns3aA6fJybPh5WnB08NC0x1TqL/rXXJ8a/Drrd9BQA08PSwXPvfAy8Msd/GnB56elsLXe1hYf/AMz36xlUOnzdt2vZtH8/ztzQvv5HxoDczqbQaJJ7YX61aYwx3fDLNugdxzZotNwp9hyQRzKAIA/zC44e/Q7sFKO5ikiDMo3BRB4UYqNGueOenknkVm2EneUfDz0NoX+un0MsfucfVs6cc2wAfdzf5HA+eYHYkdICvXytRle3nvpwPk2QyCfL34e+/G3JdQBw+PS8KcYZgTqR7bADc8Dd3GOeT4xXb2kHn+55Kh7g1w33wzwBiGeb2WjIdTu82y1etC9wnQrL+e7hIpBoWbIijcSKWScvhCP53F5tNd1kuecvIOgHrdoFFP8+mh0HjwcOIcU7lZ8O715h/vFnfDXR84/BA7T6QxbsFWNh9JAeDa2qG8fFdLGkUFXSy0bQHMH2Z2ZH5iu+vmFcs8AzN6wum9EHUNDPvWHD7gUtY82PQfs7P1uWRzXa320POfULuja+opUkEp3BRB4UYqrZxzZsDZswj2fF/ItBOB5lg6Uc3NV2Qz86ej+uwsGQ+r/g2BkTB6ndP6AlltBnPWHuK1xbvJyM7D29PCwzfUZ3S3BuZ0EtY8mNoGUg/Dbf8yZ4t3ttzz8FF/OLLWHBdpxFIIjr1y+ewMWP0mrJ568Sm5JrdB9xcgvIHz6ytSAVW4cPPWW2/x2muvkZiYSKtWrXjzzTfp0KHDVbf75JNPGDRoEP369eOLL74o1rEUbqRKMAxI3Ap7F5tB58RvBVt1LhUUUzDsRDWH8Ebg5Vv84x35BWb2NG9H3fM/aHKrY86jCCdSz/PcF9tZujMJgLrhgfzfHdfQuX44rHkbFo+DGg1g9C/ObbGyWeHTIbDza3MqjwcXmyGyONITzVacTf8xf3ceXtB2GNz4NASGO6/OIhVQhQo3c+fOZfDgwUyfPp2EhASmTJnCp59+yu7du4mMjLzidr///jtdu3alXr16hIWFKdyIFMWaB2f2mxNYJu8wfyZth5RDhZf38IIaDSGq2YXQc425HBJ3ef+Q3PMw/TrzdkzLe+DOd51/PhcYhsHi7YmM/3K7fYLQu9vW4pnucVSf3hqyU2HQJ9C4j7MqAIuehnXTzdnn719gjk9UUsk7YenzZqsbgE8QdH0cOj4CPk6a80ukgqlQ4SYhIYH27dszbdo0AGw2G3FxcfzlL3/h6aefLnQbq9XK9ddfz/Dhw/n5559JSUlRuBEpjex08w9rftjJDz5ZKYWX9w2+EHYuCT3bP4f170K1aBi91pxKwsXSsnJ5bdFu5qw7hGFAjUAfPqn7LQ33zYQ6XWHYQuccePWb8P0/zGVHTC9x8Cf4/jk4sdl8HxQLN/3DnEW9ks1cL1JSFSbc5OTkEBAQwPz58+nfv799/ZAhQ0hJSeHLL78sdLsJEyawZcsWPv/8c4YOHVpkuMnOziY7+2JzfFpaGnFxcQo3IldiGJB2/ELQ2QZJO8zlk7vBlnvl7e6dZz6O7kYbDp1h3IKt7EnKIJrTrPR7HC+s8NAKiG3j2INtnQ+fPWgu9/wndP6LY/Zrs8G2z2DZi2a/ITBDZI8XocHNjjmGSAVUknDjumFOC3Hq1CmsVitRUQVnCY6KimLXrl2FbrNy5UpmzJjB5s2bi3WMSZMm8cILL5S1qiJVh8UCITXNV8MeF9fn5cDpfRdaeLaboSdpO6QdhfYj3B5sANrWCeObv1zHez/tZ+oP+/ja2pE7PFex78uXiX/o4+LNiVUcv6+EL0aZywkPQ6cxjtkvmP2DWt5tPka//j34+XUzZM650xy0sceLEN3CcccTqYSc2MvO8dLT03nggQd4//33CQ8vXme7cePGkZqaan8dOXLEybUUqaS8fMzbUS3vhu7Pm/M3jd0O/0iGW99wd+3sfLw8GHNTQxY9dh3rogcBEJ/4PSOmfs6WoyllP0DyTvjkXnNC1aZ9oddLzhmnxtsPujwKj26GjqPBwxv2/2D2b/p8FKQec/wxL2G1GeTk2Zx6DBFncWvLTXh4OJ6eniQlJRVYn5SURHT05UOs79+/n99//52+fS8ODGazmV8+Ly8vdu/eTf369Qts4+vri69vCZ76EJGSKclTVS5UL6Iakx65n6RpHxF1ej2dT39G/7f8GNalLmN7NCLQtxT/+Us7DnP+BFmp5sjQd75foC/M+RwrSWlZ5is9m+S0LLw9PYgM8iUy2O/CT198vUrQfyYgDHq/BB1Gmreqti+A3/5n/uz4CHR5rFTTOdhsBsnp2Rw9m8mRs5kcPXOeo2fPczQlk6Nnz3M85TwWLAzrGs/jNzcq3uSlIuVEuehQ3KFDB958803ADCu1a9dmzJgxl3UozsrKYt++fQXW/eMf/yA9PZ1///vfNGrUCB+fooczV4dikSpmz2L43wDOewTSPvPfZBBAzVB//tn/Gro1ufITmZfKzrNy6tRJqs/tR8DZXaQExDO7yXscPu9LUnoWSWnZJKVlkZ6VV6z9hQZ4ExXkR2SwL5FBfkQF+xIZ5EtUsN/VQ9DRDWYn5sOrzfc+1cy5tDqOKjAx56Xh5ejZ85f8NJePpZwn11q8//zXqRHApDta0LmBHk8X96kwHYrBfBR8yJAhvPvuu3To0IEpU6Ywb948du3aRVRUFIMHD6ZmzZpMmjSp0O2v1qH4jxRuRKoYmw3eToBTe9jXZhxDd3Xg6NnzANzWMobHuzckM8dKYurF1haz9cUMLMnp2aSfy2SW9yt09dxOshHKnTkvcNQofN6qAB9PooMvBpdcq43k9Av7Sssmx1r8Wz1XDEFBvjRM+Ynam/+Fzxmzf6INT34LuYn5vnew6lwsx1OyrnosTw8LsaF+1AoNoFZ1f2pVz//pT62wAHYcT2P8l9s4kZoFmI/ZP3trU0IDNCeWuF6F6VAMMHDgQE6ePMn48eNJTEykdevWLFq0yN7J+PDhw3g4cwAuEancPDyg02j4+jEaHJjD9489zpQfDvLBzwf4ZssJvtly4io7MJjs/R5dPbeTiR+v1ZhIqxrX0CPYbGmJCva9EEDM5Wq+Xliu0AfHMAxSz+eSlJZN8iUtPifTLwapS0NQSmYuKZm57E5KL2RvQcBz3OCxhZGe39DVczttUpfQhiWstDbnPeM2Vnm0IibEDCtx1QMuCy9RQb5FdrKuGepPx3phvLZ4N/9Ze4hPNxxl+e5kJvRtzm0tY654niLu5vaWG1dTy41IFZSbBf9qDpmn7OPRbDuWynNfbmPr0VQiLvSJiQq6GFgig/2IDvaj+c4p1Ng0DcPiieXeedCwu9OrW9wQdDojhxrVfKhV3Z8EvyP0SZ9Po1NL8TCs5n4im2Hp/Chcc1eZZyDfcOgMT322lX3JGQDc1CSSif2voWaoi+bukiqvQt2WcjWFG5EqasXLsGKSOd7NyOX2J5wMw7hyC8QvH8DCv5rL/d6CNve7qLJlkHIY1r4DGz6E3HPmuqBY6PgwtB16+WSeJZCdZ+WdFft5a/k+cq0GgT6e/L13E+7vWAdPD7XiiHMp3BRB4Uakijp3ymy9ycuCod9CfJeiy+/6FubeZ875dOMzcONTrqmno5w/C7/OhHXvQsaFJ1J9gqDdUEgYZY5jVEp7k9J5esFWNhw6C0Cb2qG88sfZ2Su77AzwrebuWlQpJfn7rc4sIlI1BIZDK3PcG9ZMK7rskV9g/nAz2Fw7GG74u/Pr52j+1eG6v8LjW+H2aRDeGHLSzSkj/t0SFvwZEreVatcNo4L49M+dmNj/Gqr5erHpcAq3Tv2Zyd/vJjvP6uATKWeSd8J/B8CkmvDt380RvaXcUcuNiFQdp/bCtHaABcb8CuENLi9zej/M6AGZp6FhT7jnY/B0+7MXZWezwb4lsGoqHFp5cX39m82pI+rdWKrBCP84O3v9iEBevqsl7ePDHFTxciLtBKx4CTbNMUNvvvYj4JbXnTOQoxSg21JFULgRqeL+dw/s+Q7aDYfb/lXws4yTZrA5exBiWsPQhZXz1sOxDWYLzo4vL/6hjm4BnR+F5neAp3eJdmcYBt9tM2dnP5VhzuV3X0JtnurThGC/ku2r3MlKg9VTYfU0yDOHEKBpX6jVHpZMAAzo8BD0eVUBx8kUboqgcCNSxf2+EmbfCl5+8MQOCKxhrs85B7Nvg+MbIbQOjFgK1Yo3yF+Fdeag2fl4038gN9NcFxJnDgh47WDwLVkfmtTMXF76didzfzWnuYkK9uXFftfQq/nlI86Xe9Zc2DDb7IieecpcF5cAPSZC7QTz/ab/wpejMQPOn6HPKwo4TqRwUwSFG5EqzjDgvRvhxGbo9qzZn8aaZ3Ye3rMI/MPgwSWF37KqrDLPwC8zYP27cO6kuc43BNoOgWb9zCfMPIo//cKa/ad55vOtHDxlPq3Vu3k0L/RrTlSwnzNq71iGATu/gqUvwJn95roaDcz51Jrcdnl42TQHvhwDGOYkqr1fVsBxEoWbIijciAhb58NnD0JgBDy+DRY9Zf5fupcfDPka4jq4u4bukZsFWz4xb1mdvmSqG//qUK8bNLjZ7KMTHHPVXWXlWpm6bC/v/nQAq80gyM+LZ25pysB2cXiU18fGD6+F75+Do+vN94ERcOPT5vQWRd2q2/gf+OrCzPAJo6D3JAUcJ1C4KYLCjYhgzYV/t4a0o1C7ExxeA1hg4Bxoepu7a+d+NpvZL+m3T+DAj5CdWvDzqGug/k3QoDvU7ljk5Kk7jqfx9IItbDlq7qND3TAm3dmC+hHlqC/Tqb2w9HnY9Y353jvA7GTd+S/FvzW38SP46i/mcsdHnDdbfBWmcFMEhRsRAczWie//cfH9La+bM29LQdY8swPyvqXm6/gm4JI/G96BUPc6M+jUvwlq1L98FzaDWasO8sb3ezifa8XHy4NHb2rAQ9fXx8er+COSGIaBzbj402YYGAYYXPLeZr43DAjx9y66lSgj2exTs2E2GFaweECbB6DbMxBUin5CGz6Erx81lzuOhl7/p4DjQAo3RVC4EREAslLhX9dAdhp0eRx6vODuGlUM507DgeWwbxnsX3ZxgMB81euat68adIf46wo8bXbkTCbPfrGNn/aY/XoCfTzx8vTAuBBSbPnhhYIhJv9nSQX7edGhbg061gsjoW4NmsUGmyMp55wzn35aPRVyzOkkaNTH7FcT2aSUv5gLNsyGrx8zlzuNgZ7/VMBxEIWbIijciIjdodXmLYk2D5gTbErJGAYkbbvQqrPM7LNiy734uYe3eduqQXcz8ERdgwF8sfkYL369g7OZuVfctTOE+lp4PHw9d6f/h8CcC09AxV4LPSdCfFfHHejXWfDN4+ayAo7DKNwUQeFGRMRJstPNR+3zb2Gd/b3g59WiL/TVuZnzcTdwNNsPi8WChwX7Tw+LBcsf33OVz//4E7AZsONEGusOnGbdgdME/L6Ex4z/0tDjGACHbJH8m3s5XecWEurXIKFuDVrWCsG7iFnSS+TXmfDNE+Zy57+Yj5Ar4JSJwk0RFG5ERFzk9P6Lt68O/nRxLB0ALOYj1gE1ICDMfAQ/oLr5ZJZ/2IV1f1j2LsUM5Ec3wJLn4NAqALK8Q/k86D5eO9OFM1kFi/p7e9IuvjoJdcNIqGeGHV+v4j8Cf5lfZsDCseZy50ehx4tuDTg5eTa2H09l0+EUqvl5cWuLGAJ9K87o2wo3RVC4ERFxg7xs87bVvqWw/wfzdlZJeflfEnqqXwxFhS17eJtziG1fcGFbP3Nwwi6Pg38oVpvBrsQ01h04w7qDp1l/8Mxlt8l8vTy4tnZ1OtarQUK9MFrHheLnXcKwc+nM8l0eg+4vuCzgnEzPZuPhs2w8dJYNh86y5VgqOXkXp44I8vXirra1eKBTnfL19NoVKNwUQeFGRKQcSDsBp/eaAwieP2POYp554ad9+ZL1Rmkn5LRA63vNJ6BCal2xlM1msDc5g7UHTrPu4GnWHTjD6XM5Bcr4eHnQOi7UDDt1w2gQWY2Iar5XH7dn/fvw7ZPmcpfHzY7LDg44VpvB7sR0Nhw+y6ZDZ9lw+CyHTmdeVq56gDdtalfn4Klz9kEWAbo2COeBTnW4uUkkXo66NedgCjdFULgREalgDMN8qq1A6EkpJBhdspyVas7/dPN4iL6mFIc02H8yg7UHzlwIPGc4mZ59WTkfLw9qhfpTKyyAuOr+xIUFEFc9gLgwf+KqBxAa4I3FYikYcLo+ATdPKFPAST2fy6bDZ9l4OIWNh86y6fBZzuUUDIAWCzSKDOLaOtVpW6c619YOpW54IBaLBZvNYOW+U3y05neW7Uq2T25eM9SfexNqc0/7OGpUu/L4Re6gcFMEhRsRESkpwzA4eOoc6w6eYd2B02w8nMKxlPNYr/KMejVfL2pdCD135H7LLUfeAOBUm9H493qBwGJMLJp/7A2HzrLxsHmLaW9yBn/8613N14s2tUNpU9sMM63jQgnxL2T/51PMn/6hgPmI/n/XHWbuL4ftt+Z8PD24rWUMD3SqQ+u4UDOguZnCTREUbkRExBFyrTYSU7M4ciaTI2czOXLm/IWfmRw9e57kQlp6Bnsu5kXvDwGYltePmT73ExcWcKHlx2zxqVU9AG9PC5sutMpsPHy20Mfm69QIoG3t6vaWmUZRQeY4PoU5d8qcM2v7F+YTbQB1r4dr7jTnzAoIIyvXyjdbTvCfNb/z29GLo1K3qBnC4E516NsqtuR9jhxI4aYICjciIuIKWblWjp41A8/RM5kcOXueI2cyaXV8Lg9nvgvAm3n9eSPvbqDolhEfLw9a1Qoxg8yFQBN+tdtGGSdh19cXAs3PYNgKL+fhDfW7QfM7ockt4BfC5iMpfLTmd77ZcsLeCTk0wJuB7eK4v2Md4sICSvjbKDuFmyIo3IiIiNutfQcWPQ3A/qaPsDxmBEdTLrYCZeZYaVUrlDa1Q2lbpzrNY0OKN1VFxkmzhWbHF2YLzaWBJqY1NO9vzvRuGOaTZNu/KPjkmqePOehi8zuhcW/O5Pky95cjzFl7iGMp5wGzL89NjSN5oFMdrm8Y4bKJUBVuiqBwIyIi5cKat2HxOHP5hqfMJ7pKIyP54i2nQ6sKBprYNtCsvxlowuoWvv3J3bD9c9i2AE7tvrjeyw8a9oRr7sTaoCc/7M/gozW/8/PeU/Yi8TUCuL9jHe5uG0dIwNX7D5WFwk0RFG5ERKTcWPMWLL4Qam54GrqNK9526UkXWmi+LCTQXHuxhaZ6fPHrYhiQvMMMOdsXwJkDFz/zDoTGvaH5HRwM7cRHvyYxf8NR0rPyAPDz9qB/65o80KkOzWNDin/MElC4KYLCjYiIlCurp8H3z5rLN46DG58uvFx+oMlvobl0dvbSBporMQw48ZvZorN9AaQcvviZTxA0uYWsRv34PL0xH647zq7EdPvH7epU54FOdbitZeyVOziXgsJNERRuRESk3Fn9Jnz/D3P5xmfgxqfM5fRE2HGhD82h1RQINDXbXrzlVL2O8+pmGHBs44U+Op9D2rGLn/mFYDS5jT3hPZh2qCbfbT9Fns2gXkQgy8be4NBHyBVuiqBwIyIi5dKqqeY8WACt7jUnHj28hoKBpt3FFprQ2q6vo80GR9dfaNH5AjISL37mH0Zmg1v4zugE8ddxVzvHBi6FmyIo3IiISLm16t+wZHzBdbXaX2yhCY1zS7UKZbOa4WvbArPvT+bFjsaE1Ycxv4KH46ZyKMnf74ozHaiIiEhl1+Ux8AmEXQvNR7Kb9StyTiy38vCE+K7mq8+r5lg62xfAzq+hVjuHBpuSUsuNiIiIOI41F7LSILCGQ3dbkr/f5XPqTxEREamYPL0dHmxKSuFGREREKhWFGxEREalUFG5ERESkUlG4ERERkUpF4UZEREQqFYUbERERqVQUbkRERKRSUbgRERGRSkXhRkRERCoVhRsRERGpVBRuREREpFJRuBEREZFKReFGREREKhUvd1fA1QzDAMyp00VERKRiyP+7nf93vChVLtykp6cDEBcX5+aaiIiISEmlp6cTEhJSZBmLUZwIVInYbDaOHz9OUFAQFovFoftOS0sjLi6OI0eOEBwc7NB9lzc618qrKp2vzrXyqkrnW1XO1TAM0tPTiY2NxcOj6F41Va7lxsPDg1q1ajn1GMHBwZX6H9ildK6VV1U6X51r5VWVzrcqnOvVWmzyqUOxiIiIVCoKNyIiIlKpKNw4kK+vLxMmTMDX19fdVXE6nWvlVZXOV+daeVWl861K51pcVa5DsYiIiFRuarkRERGRSkXhRkRERCoVhRsRERGpVBRuREREpFJRuCmht956i/j4ePz8/EhISGD9+vVFlv/0009p0qQJfn5+tGjRgm+//dZFNS29SZMm0b59e4KCgoiMjKR///7s3r27yG1mz56NxWIp8PLz83NRjcvm+eefv6zuTZo0KXKbinhdAeLj4y87V4vFwujRowstX5Gu608//UTfvn2JjY3FYrHwxRdfFPjcMAzGjx9PTEwM/v7+dO/enb179151vyX9zrtKUeebm5vLU089RYsWLQgMDCQ2NpbBgwdz/PjxIvdZmu+CK1zt2g4dOvSyevfu3fuq+y2P1/Zq51rY99disfDaa69dcZ/l9bo6k8JNCcydO5exY8cyYcIENm7cSKtWrejVqxfJycmFll+9ejWDBg3iwQcfZNOmTfTv35/+/fuzbds2F9e8ZH788UdGjx7N2rVrWbJkCbm5ufTs2ZNz584VuV1wcDAnTpywvw4dOuSiGpdd8+bNC9R95cqVVyxbUa8rwC+//FLgPJcsWQLA3XfffcVtKsp1PXfuHK1ateKtt94q9PNXX32VqVOnMn36dNatW0dgYCC9evUiKyvrivss6XfelYo638zMTDZu3Mhzzz3Hxo0bWbBgAbt37+b222+/6n5L8l1wlatdW4DevXsXqPfHH39c5D7L67W92rleeo4nTpxg5syZWCwW7rrrriL3Wx6vq1MZUmwdOnQwRo8ebX9vtVqN2NhYY9KkSYWWHzBggHHrrbcWWJeQkGD8+c9/dmo9HS05OdkAjB9//PGKZWbNmmWEhIS4rlIONGHCBKNVq1bFLl9ZrqthGMZjjz1m1K9f37DZbIV+XlGvK2B8/vnn9vc2m82Ijo42XnvtNfu6lJQUw9fX1/j444+vuJ+Sfufd5Y/nW5j169cbgHHo0KErlinpd8EdCjvXIUOGGP369SvRfirCtS3Ode3Xr59x0003FVmmIlxXR1PLTTHl5OSwYcMGunfvbl/n4eFB9+7dWbNmTaHbrFmzpkB5gF69el2xfHmVmpoKQFhYWJHlMjIyqFOnDnFxcfTr14/t27e7onoOsXfvXmJjY6lXrx733Xcfhw8fvmLZynJdc3JymDNnDsOHDy9yEtmKfF3zHTx4kMTExALXLSQkhISEhCtet9J858uz1NRULBYLoaGhRZYryXehPFmxYgWRkZE0btyYUaNGcfr06SuWrSzXNikpiYULF/Lggw9etWxFva6lpXBTTKdOncJqtRIVFVVgfVRUFImJiYVuk5iYWKLy5ZHNZuPxxx+nS5cuXHPNNVcs17hxY2bOnMmXX37JnDlzsNlsdO7cmaNHj7qwtqWTkJDA7NmzWbRoEe+88w4HDx7kuuuuIz09vdDyleG6AnzxxRekpKQwdOjQK5apyNf1UvnXpiTXrTTf+fIqKyuLp556ikGDBhU5sWJJvwvlRe/evfnoo49YtmwZr7zyCj/++CN9+vTBarUWWr6yXNsPP/yQoKAg7rzzziLLVdTrWhZVblZwKZnRo0ezbdu2q96f7dSpE506dbK/79y5M02bNuXdd99l4sSJzq5mmfTp08e+3LJlSxISEqhTpw7z5s0r1v8RVVQzZsygT58+xMbGXrFMRb6uYsrNzWXAgAEYhsE777xTZNmK+l2455577MstWrSgZcuW1K9fnxUrVnDzzTe7sWbONXPmTO67776rdvKvqNe1LNRyU0zh4eF4enqSlJRUYH1SUhLR0dGFbhMdHV2i8uXNmDFj+Oabb1i+fDm1atUq0bbe3t60adOGffv2Oal2zhMaGkqjRo2uWPeKfl0BDh06xNKlSxkxYkSJtquo1zX/2pTkupXmO1/e5AebQ4cOsWTJkiJbbQpzte9CeVWvXj3Cw8OvWO/KcG1//vlndu/eXeLvMFTc61oSCjfF5OPjQ9u2bVm2bJl9nc1mY9myZQX+z/ZSnTp1KlAeYMmSJVcsX14YhsGYMWP4/PPP+eGHH6hbt26J92G1Wtm6dSsxMTFOqKFzZWRksH///ivWvaJe10vNmjWLyMhIbr311hJtV1Gva926dYmOji5w3dLS0li3bt0Vr1tpvvPlSX6w2bt3L0uXLqVGjRol3sfVvgvl1dGjRzl9+vQV613Rry2YLa9t27alVatWJd62ol7XEnF3j+aK5JNPPjF8fX2N2bNnGzt27DAeeughIzQ01EhMTDQMwzAeeOAB4+mnn7aXX7VqleHl5WW8/vrrxs6dO40JEyYY3t7extatW911CsUyatQoIyQkxFixYoVx4sQJ+yszM9Ne5o/n+sILLxiLFy829u/fb2zYsMG45557DD8/P2P79u3uOIUS+etf/2qsWLHCOHjwoLFq1Sqje/fuRnh4uJGcnGwYRuW5rvmsVqtRu3Zt46mnnrrss4p8XdPT041NmzYZmzZtMgBj8uTJxqZNm+xPB7388stGaGio8eWXXxpbtmwx+vXrZ9StW9c4f/68fR833XST8eabb9rfX+07705FnW9OTo5x++23G7Vq1TI2b95c4HucnZ1t38cfz/dq3wV3Kepc09PTjSeffNJYs2aNcfDgQWPp0qXGtddeazRs2NDIysqy76OiXNur/Ts2DMNITU01AgICjHfeeafQfVSU6+pMCjcl9Oabbxq1a9c2fHx8jA4dOhhr1661f3bDDTcYQ4YMKVB+3rx5RqNGjQwfHx+jefPmxsKFC11c45IDCn3NmjXLXuaP5/r444/bfy9RUVHGLbfcYmzcuNH1lS+FgQMHGjExMYaPj49Rs2ZNY+DAgca+ffvsn1eW65pv8eLFBmDs3r37ss8q8nVdvnx5of9u88/HZrMZzz33nBEVFWX4+voaN99882W/gzp16hgTJkwosK6o77w7FXW+Bw8evOL3ePny5fZ9/PF8r/ZdcJeizjUzM9Po2bOnERERYXh7ext16tQxRo4ceVlIqSjX9mr/jg3DMN59913D39/fSElJKXQfFeW6OpPFMAzDqU1DIiIiIi6kPjciIiJSqSjciIiISKWicCMiIiKVisKNiIiIVCoKNyIiIlKpKNyIiIhIpaJwIyIiIpWKwo2IVHkrVqzAYrGQkpLi7qqIiAMo3IiIiEilonAjIiIilYrCjYi4nc1mY9KkSdStWxd/f39atWrF/PnzgYu3jBYuXEjLli3x8/OjY8eObNu2rcA+PvvsM5o3b46vry/x8fG88cYbBT7Pzs7mqaeeIi4uDl9fXxo0aMCMGTMKlNmwYQPt2rUjICCAzp07s3v3bueeuIg4hcKNiLjdpEmT+Oijj5g+fTrbt2/niSee4P777+fHH3+0l/nb3/7GG2+8wS+//EJERAR9+/YlNzcXMEPJgAEDuOeee9i6dSvPP/88zz33HLNnz7ZvP3jwYD7++GOmTp3Kzp07effdd6lWrVqBejz77LO88cYb/Prrr3h5eTF8+HCXnL+IOJYmzhQRt8rOziYsLIylS5fSqVMn+/oRI0aQmZnJQw89RLdu3fjkk08YOHAgAGfOnKFWrVrMnj2bAQMGcN9993Hy5Em+//57+/Z///vfWbhwIdu3b2fPnj00btyYJUuW0L1798vqsGLFCrp168bSpUu5+eabAfj222+59dZbOX/+PH5+fk7+LYiII6nlRkTcat++fWRmZtKjRw+qVatmf3300Ufs37/fXu7S4BMWFkbjxo3ZuXMnADt37qRLly4F9tulSxf27t2L1Wpl8+bNeHp6csMNNxRZl5YtW9qXY2JiAEhOTi7zOYqIa3m5uwIiUrVlZGQAsHDhQmrWrFngM19f3wIBp7T8/f2LVc7b29u+bLFYALM/kIhULGq5ERG3atasGb6+vhw+fJgGDRoUeMXFxdnLrV271r589uxZ9uzZQ9OmTQFo2rQpq1atKrDfVatW0ahRIzw9PWnRogU2m61AHx4RqbzUciMibhUUFMSTTz7JE088gc1mo2vXrqSmprJq1SqCg4OpU6cOAC+++CI1atQgKiqKZ599lvDwcPr37w/AX//6V9q3b8/EiRMZOHAga9asYdq0abz99tsAxMfHM2TIEIYPH87UqVNp1aoVhw4dIjk5mQEDBrjr1EXESRRuRMTtJk6cSEREBJMmTeLAgQOEhoZy7bXX8swzz9hvC7388ss89thj7N27l9atW/P111/j4+MDwLXXXsu8efMYP348EydOJCYmhhdffJGhQ4faj/HOO+/wzDPP8Mgjj3D69Glq167NM888447TFREn09NSIlKu5T/JdPbsWUJDQ91dHRGpANTnRkRERCoVhRsRERGpVHRbSkRERCoVtdyIiIhIpaJwIyIiIpWKwo2IiIhUKgo3IiIiUqko3IiIiEilonAjIiIilYrCjYiIiFQqCjciIiJSqSjciIiISKXy//zF/sDg9dxQAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.plot(val_history)\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('Learning curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
